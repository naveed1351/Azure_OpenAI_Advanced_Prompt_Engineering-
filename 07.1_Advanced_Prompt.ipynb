{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "49ad43d8",
   "metadata": {},
   "source": [
    "# üéì Advanced Prompt Engineering with Azure OpenAI & GPT-4.1  \n",
    "  \n",
    "> In this hands-on interactive notebook, you'll learn and apply best practices for advanced prompt engineering using **GPT-4.1** via the **Azure OpenAI** platform.  \n",
    "  \n",
    "---  \n",
    "  \n",
    "## 1Ô∏è‚É£ Setup: Azure OpenAI Client  \n",
    "  \n",
    "We'll load environment variables from a local file and create our Azure OpenAI client.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edc7b70f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1Ô∏è‚É£ Setup cell: Import libraries and load environment variables  \n",
    "  \n",
    "import os  \n",
    "from openai import AzureOpenAI  \n",
    "from dotenv import load_dotenv  \n",
    "  \n",
    "# Load Azure credentials from a .env file (you should have credentials.env in your directory)  \n",
    "load_dotenv(\"credentials.env\")  \n",
    "  \n",
    "# Create Azure OpenAI client  \n",
    "client = AzureOpenAI(  \n",
    "    api_key=os.getenv(\"AZURE_OPENAI_KEY\"),  \n",
    "    api_version=\"2025-01-01-preview\",  \n",
    "    azure_endpoint=os.getenv(\"AZURE_OPENAI_ENDPOINT\")  \n",
    ")  \n",
    "  \n",
    "print(\"‚úÖ Azure OpenAI client initialized.\")  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddb88168",
   "metadata": {},
   "source": [
    "## 2Ô∏è‚É£ Your First Prompt: Hello, GPT-4.1!  \n",
    "  \n",
    "Let's run a *simple chat completion* to test the setup and confirm our credentials.  \n",
    "  \n",
    "**Instructions:** Fill in your deployment name as configured in Azure OpenAI Portal (replace `'YOUR_DEPLOYMENT_NAME'`).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13524725",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2Ô∏è‚É£ First prompt cell: Basic chat completion  \n",
    "  \n",
    "deployment_name = \"gpt-4.1\"  # <-- CHANGE THIS!  \n",
    "  \n",
    "response = client.chat.completions.create(  \n",
    "    model=deployment_name,  \n",
    "    messages = [  \n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},  \n",
    "        {\"role\": \"user\", \"content\": \"Hello! What can you do?\"}  \n",
    "    ]  \n",
    ")  \n",
    "  \n",
    "print(response.choices[0].message.content)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc553899",
   "metadata": {},
   "source": [
    "---  \n",
    "## 3Ô∏è‚É£ Advanced: Chain of Thought Prompting  \n",
    "  \n",
    "Instruct the model to show its reasoning step by step. This is called \"chain of thought\" (CoT) and is **very effective** with GPT-4.1.  \n",
    "  \n",
    "Try solving a math riddle or similar question.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4fabca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3Ô∏è‚É£ Chain-of-thought demo cell  \n",
    "  \n",
    "question = \"If Alice has twice as many apples as Bob, and together they have 12, how many apples does Alice have?\"  \n",
    "  \n",
    "response = client.chat.completions.create(  \n",
    "    model=deployment_name,  \n",
    "    messages = [  \n",
    "        {\"role\": \"system\", \"content\": \"You are an expert math tutor. Show your reasoning step by step before giving an answer.\"},  \n",
    "        {\"role\": \"user\", \"content\": question}  \n",
    "    ]  \n",
    ")  \n",
    "print(response.choices[0].message.content)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c7c1fb0",
   "metadata": {},
   "source": [
    "---  \n",
    "## 4Ô∏è‚É£: Building Instruction-Heavy, Agentic Prompts  \n",
    "  \n",
    "GPT-4.1 follows instructions *literally* and *precisely*. Let's see how adding rules changes the response.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26840902",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4Ô∏è‚É£ Instruction-heavy prompt  \n",
    "  \n",
    "prompt = \"\"\"  \n",
    "You are a goal-oriented agent. Always:  \n",
    "- Greet the user.  \n",
    "- Explain your reasoning step by step before your answer.  \n",
    "- If you don't know, say so directly.  \n",
    "- Be concise.  \n",
    "\"\"\"  \n",
    "  \n",
    "question = \"What is the capital of Australia, and explain why?\"  \n",
    "  \n",
    "response = client.chat.completions.create(  \n",
    "    model=deployment_name,  \n",
    "    messages = [  \n",
    "        {\"role\": \"system\", \"content\": prompt},  \n",
    "        {\"role\": \"user\", \"content\": question}  \n",
    "    ]  \n",
    ")  \n",
    "print(response.choices[0].message.content)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12325a99",
   "metadata": {},
   "source": [
    "---  \n",
    "  \n",
    "## 5Ô∏è‚É£ Agentic Workflows: Planning, Persistence & Tool Use  \n",
    "  \n",
    "GPT-4.1 excels when treated as an agent with clear multi-step objectives.  \n",
    "In agentic prompts, *always include*:  \n",
    "- **Persistence**: Instruct the model to keep going until the problem is solved  \n",
    "- **Tool use**: Be explicit about tool access and not making up answers  \n",
    "- **Planning/Reflection**: Tell the model to plan before and after each action  \n",
    "  \n",
    "Below is a system prompt pattern using all three, for coding, support, or research agents.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c328efb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5Ô∏è‚É£ Example Agentic System Prompt (for coding agent)  \n",
    "  \n",
    "SYS_PROMPT_AGENT = \"\"\"  \n",
    "You are an autonomous agent helping a developer fix code.  \n",
    "- Continue working step by step until the issue is truly solved.  \n",
    "- Only stop if you are certain the problem is fully resolved.  \n",
    "- Use your file/tools to gather context ‚Äì NEVER guess.  \n",
    "- For every function/tool call, PLAN before calling it and REFLECT after you get the result‚Äîdescribe your plan and outcome each time.  \n",
    "\"\"\"  \n",
    "  \n",
    "user_issue = \"The function process_data throws a TypeError in some cases. Find and fix the bug.\"  \n",
    "  \n",
    "response = client.chat.completions.create(  \n",
    "    model=\"gpt-4.1\",  # <-- CHANGE THIS!\n",
    "    messages = [  \n",
    "        {\"role\": \"system\", \"content\": SYS_PROMPT_AGENT},  \n",
    "        {\"role\": \"user\", \"content\": user_issue}  \n",
    "    ]  \n",
    ")  \n",
    "  \n",
    "print(response.choices[0].message.content)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2825e6e8",
   "metadata": {},
   "source": [
    "---  \n",
    "  \n",
    "## 6Ô∏è‚É£ Best Practices for Tool Use  \n",
    "  \n",
    "When connecting GPT-4.1 to external tools/functions:  \n",
    "- **Always use the tool/function schema, not in-prompt descriptions**  \n",
    "- Clear names and parameter descriptions  \n",
    "- Place tool usage examples in an `# Examples` section of your prompt  \n",
    "  \n",
    "The example cell below uses a (simulated) lookup tool‚Äîadapt for your API.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "162f99b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json  \n",
    "  \n",
    "# 6Ô∏è‚É£ Correct Tool schema for Azure OpenAI, including the full tool call cycle  \n",
    "  \n",
    "tools = [{  \n",
    "    \"type\": \"function\",  \n",
    "    \"function\": {  \n",
    "        \"name\": \"lookup_product\",  \n",
    "        \"description\": \"Finds product info by product_id.\",  \n",
    "        \"parameters\": {  \n",
    "            \"type\": \"object\",  \n",
    "            \"properties\": {  \n",
    "                \"product_id\": {\"type\": \"string\", \"description\": \"Unique product identifier.\"}  \n",
    "            },  \n",
    "            \"required\": [\"product_id\"],  \n",
    "        },  \n",
    "    }  \n",
    "}]  \n",
    "  \n",
    "system = (  \n",
    "    \"You are a product expert. Never guess product details: \"  \n",
    "    \"call the lookup_product tool whenever you need information you don't see.\"  \n",
    "    \"\\n# Examples\"  \n",
    "    \"\\nUser: What are the specs for product X123?\"  \n",
    "    \"\\nAssistant: I'll use the lookup_product tool for product X123.\"  \n",
    ")  \n",
    "user = \"What is the warranty period for Y789?\"  \n",
    "  \n",
    "# Step 1: Assistant receives question and calls the tool  \n",
    "response = client.chat.completions.create(  \n",
    "    model=\"gpt-4.1\",  # or use your string e.g. \"gpt-4.1\"  \n",
    "    messages=[  \n",
    "        {\"role\": \"system\", \"content\": system},  \n",
    "        {\"role\": \"user\", \"content\": user}  \n",
    "    ],  \n",
    "    tools=tools,  \n",
    "    tool_choice=\"auto\"  \n",
    ")  \n",
    "  \n",
    "assistant_msg = response.choices[0].message  \n",
    "  \n",
    "# Step 2: Detect tool call and simulate tool's result  \n",
    "if assistant_msg.tool_calls:  \n",
    "    tool_call = assistant_msg.tool_calls[0]  \n",
    "    args = json.loads(tool_call.function.arguments)  \n",
    "    product_id = args[\"product_id\"]  \n",
    "  \n",
    "    # Simulated \"database\" lookup  \n",
    "    tool_result = f\"Warranty for {product_id} is 24 months.\"  \n",
    "  \n",
    "    # Step 3: Return result as a tool message, then get final assistant response  \n",
    "    followup = client.chat.completions.create(  \n",
    "        model=\"gpt-4.1\",  # <-- CHANGE THIS!\n",
    "        messages=[  \n",
    "            {\"role\": \"system\", \"content\": system},  \n",
    "            {\"role\": \"user\", \"content\": user},  \n",
    "            {  \n",
    "                \"role\": \"assistant\",  \n",
    "                \"content\": None,  \n",
    "                \"tool_calls\": [tool_call]  \n",
    "            },  \n",
    "            {  \n",
    "                \"role\": \"tool\",  \n",
    "                \"tool_call_id\": tool_call.id,  \n",
    "                \"name\": \"lookup_product\",  \n",
    "                \"content\": tool_result  \n",
    "            },  \n",
    "        ],  \n",
    "        tools=tools  \n",
    "    )  \n",
    "    print(followup.choices[0].message.content)  \n",
    "else:  \n",
    "    print(assistant_msg.content)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ef81cdc",
   "metadata": {},
   "source": [
    "---  \n",
    "  \n",
    "## 7Ô∏è‚É£ Long Context: Handling Large Inputs  \n",
    "  \n",
    "GPT-4.1 supports up to **1 million tokens** in context. Strong structuring and clear delimiting improves retrieval and relevance when working with many files, docs, or data.  \n",
    "  \n",
    "**Best practices:**  \n",
    "- Use Markdown or XML/HTML tags to delimit documents.  \n",
    "- Repeat key instructions before and after large context blocks.  \n",
    "- Tell the model whether to answer with external context only, or to combine it with its own knowledge.  \n",
    "  \n",
    "Below is an example pattern using XML for document injection with clear instructions.  \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a877fbcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_longctx = (  \n",
    "    \"You are a document retrieval assistant. Only answer using the provided document context.\"  \n",
    "    \"\\nIf answer cannot be found, say 'I don't have the information needed to answer that.'\"  \n",
    "    \"\\n# External Context START\\n\"  \n",
    "    \"<doc id=1 title='Warranty Policy'>Product Y789 carries a 24-month warranty from purchase date.</doc>\\n\"  \n",
    "    \"<doc id=2 title='Returns Policy'>Returns accepted within 30 days of purchase.</doc>\\n\"  \n",
    "    \"# External Context END\\n\"  \n",
    "    \"Restate your instructions: only use the provided documents!\"  \n",
    ")  \n",
    "user = \"What is the warranty for Y789?\"  \n",
    "  \n",
    "response = client.chat.completions.create(  \n",
    "    model=\"gpt-4.1\",  # <-- CHANGE THIS!\n",
    "    messages=[  \n",
    "        {\"role\": \"system\", \"content\": system_longctx},  \n",
    "        {\"role\": \"user\", \"content\": user},  \n",
    "    ]  \n",
    ")  \n",
    "print(response.choices[0].message.content)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09e78461",
   "metadata": {},
   "source": [
    "---  \n",
    "  \n",
    "## 8Ô∏è‚É£ Inducing Explicit Planning (Chain of Thought)  \n",
    "  \n",
    "Prompting the model to **plan or explain step by step** can improve complex outputs.  \n",
    "  \n",
    "**When to use:**    \n",
    "- Multi-hop document answers  \n",
    "- Code debugging/patching  \n",
    "- Tasks needing structured reasoning  \n",
    "  \n",
    "**Pattern:**  \n",
    "List reasoning/investigation steps, and prompt the model to output them.  \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b173461",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_cot = (  \n",
    "    \"You are a senior support engineer.\"  \n",
    "    \"\\nFirst, break down the user's question. Second, review possible relevant documents. Third, explain your reasoning step by step before answering.\"  \n",
    ")  \n",
    "  \n",
    "user = (  \n",
    "    \"Does Y789 qualify for free extended support if it's out of warranty but only by a week? Use policy documents if available.\"  \n",
    ")  \n",
    "  \n",
    "response = client.chat.completions.create(  \n",
    "    model=\"gpt-4.1\",  # <-- CHANGE THIS!\n",
    "    messages=[  \n",
    "        {\"role\": \"system\", \"content\": system_cot},  \n",
    "        {\"role\": \"user\", \"content\": user},  \n",
    "    ]  \n",
    ")  \n",
    "print(response.choices[0].message.content)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d5f5f05",
   "metadata": {},
   "source": [
    "---  \n",
    "  \n",
    "## 9Ô∏è‚É£ Diffs and Patch Generation  \n",
    "  \n",
    "GPT-4.1 excels at generating file/code diffs and producing precise patch outputs. Leveraging standardized formats improves reliability and makes integration with downstream tools (like `git apply` or patching libraries) much easier.  \n",
    "  \n",
    "**Best practices:**  \n",
    "- Ask the model to generate diffs in a format compatible with Unix `patch` or common diff tools.  \n",
    "- Provide the context (old file, new requirement/instruction) and ask for a Unix-style or unified diff.  \n",
    "- If you want the model to only output the diff, **explicitly request ‚ÄúOutput ONLY the diff, and nothing else.‚Äù**  \n",
    "- For automated pipelines: check result formatting and wrap output in code blocks.  \n",
    "  \n",
    "---  \n",
    "  \n",
    "#### Example: Code Patch with Unified Diff  \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83c41976",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_patch = (  \n",
    "    \"You are a patch generation tool. Only output valid unified diffs in code blocks.\\n\"  \n",
    "    \"When given an old file and a description of a required change, generate the unified diff.\\n\"  \n",
    "    \"Do NOT add any explanations or extra text.\\n\"  \n",
    "    \"\\n\"  \n",
    "    \"Example input:\\n\"  \n",
    "    \"[Old file]:\\n\"  \n",
    "    \"def add(a, b):\\n\"  \n",
    "    \"    return a + b\\n\"  \n",
    "    \"\\n\"  \n",
    "    \"[Instruction]: Add input type checks so only integers are allowed.\\n\"  \n",
    "    \"----\\n\"  \n",
    "    \"Example output:\\n\"  \n",
    "    \"```diff\\n\"  \n",
    "    \"--- original.py\\n\"  \n",
    "    \"+++ modified.py\\n\"  \n",
    "    \"@@ -1,3 +1,7 @@\\n\"  \n",
    "    \" def add(a, b):\\n\"  \n",
    "    \"+    if not (isinstance(a, int) and isinstance(b, int)):\\n\"  \n",
    "    \"+        raise TypeError('Inputs must be integers')\\n\"  \n",
    "    \"     return a + b\\n\"  \n",
    "    \"```\"  \n",
    ")  \n",
    "  \n",
    "user_patch = (  \n",
    "    \"[Old file]:\\n\"  \n",
    "    \"def calculate_price(base, tax):\\n\"  \n",
    "    \"    return base * (1 + tax)\\n\"  \n",
    "    \"\\n\"  \n",
    "    \"[Instruction]: Prevent negative base prices. Raise ValueError if base < 0.\"  \n",
    ")  \n",
    "  \n",
    "response = client.chat.completions.create(  \n",
    "    model=\"gpt-4.1\",  # <-- CHANGE THIS!\n",
    "    messages=[  \n",
    "        {\"role\": \"system\", \"content\": system_patch},  \n",
    "        {\"role\": \"user\", \"content\": user_patch},  \n",
    "    ]  \n",
    ")  \n",
    "print(response.choices[0].message.content)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d71de2c",
   "metadata": {},
   "source": [
    "---  \n",
    "  \n",
    "### ‚úÖ Pro Tips:  \n",
    "- For file patching tasks, always clearly delimit old content, instruction, and desired output format.  \n",
    "- For multi-file or more complex changes, explicitly ask for a diff per file.  \n",
    "- Use Markdown code blocks for reliable parsing and downstream processing.  \n",
    "  \n",
    "---  \n",
    "  \n",
    "## üìö End of Notebook  \n",
    "  \n",
    "You now have a complete hands-on notebook covering all primary OpenAI GPT-4.1 prompt engineering patterns: agentic workflows, function calls, chain-of-thought, long context, formatting, and patch/diff generation!  \n",
    "  \n",
    "If you need full worked examples or want to add advanced topics (like tool chaining or RAG best practices), let me know!  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
