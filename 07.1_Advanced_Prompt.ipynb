{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "49ad43d8",
   "metadata": {},
   "source": [
    "# üéì Advanced Prompt Engineering with Azure OpenAI & GPT-4.1  \n",
    "  \n",
    "> In this hands-on interactive notebook, you'll learn and apply best practices for advanced prompt engineering using **GPT-4.1** via the **Azure OpenAI** platform.  \n",
    "  \n",
    "---  \n",
    "  \n",
    "## 1Ô∏è‚É£ Setup: Azure OpenAI Client  \n",
    "  \n",
    "We'll load environment variables from a local file and create our Azure OpenAI client.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "edc7b70f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Azure OpenAI client initialized.\n"
     ]
    }
   ],
   "source": [
    "# 1Ô∏è‚É£ Setup cell: Import libraries and load environment variables  \n",
    "  \n",
    "import os  \n",
    "from openai import AzureOpenAI  \n",
    "from dotenv import load_dotenv  \n",
    "  \n",
    "# Load Azure credentials from a .env file (you should have credentials.env in your directory)  \n",
    "load_dotenv(\"credentials.env\")  \n",
    "  \n",
    "# Create Azure OpenAI client  \n",
    "client = AzureOpenAI(  \n",
    "    api_key=os.getenv(\"AZURE_OPENAI_KEY\"),  \n",
    "    api_version=\"2025-01-01-preview\",  \n",
    "    azure_endpoint=os.getenv(\"AZURE_OPENAI_ENDPOINT\")  \n",
    ")  \n",
    "  \n",
    "print(\"‚úÖ Azure OpenAI client initialized.\")  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddb88168",
   "metadata": {},
   "source": [
    "## 2Ô∏è‚É£ Your First Prompt: Hello, GPT-4.1!  \n",
    "  \n",
    "Let's run a *simple chat completion* to test the setup and confirm our credentials.  \n",
    "  \n",
    "**Instructions:** Fill in your deployment name as configured in Azure OpenAI Portal (replace `'YOUR_DEPLOYMENT_NAME'`).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "13524725",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello! I can help with a wide range of tasks, such as:\n",
      "\n",
      "- Answering questions on many topics (science, history, technology, etc.)\n",
      "- Assisting with writing (essays, emails, stories, resumes, etc.)\n",
      "- Summarizing or explaining complex information\n",
      "- Creating study guides or helping with homework\n",
      "- Brainstorming ideas (projects, gifts, content, etc.)\n",
      "- Providing coding help or debugging code\n",
      "- Conversational practice or role-playing scenarios\n",
      "- Giving recommendations (books, movies, music, etc.)\n",
      "- Helping structure plans (travel, events, schedules)\n",
      "- Offering advice or resources on learning new skills\n",
      "\n",
      "If you have something specific in mind, just ask! What would you like help with today?\n"
     ]
    }
   ],
   "source": [
    "# 2Ô∏è‚É£ First prompt cell: Basic chat completion  \n",
    "  \n",
    "deployment_name = \"gpt-4.1\"  # <-- CHANGE THIS!  \n",
    "  \n",
    "response = client.chat.completions.create(  \n",
    "    model=deployment_name,  \n",
    "    messages = [  \n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},  \n",
    "        {\"role\": \"user\", \"content\": \"Hello! What can you do?\"}  \n",
    "    ]  \n",
    ")  \n",
    "  \n",
    "print(response.choices[0].message.content)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc553899",
   "metadata": {},
   "source": [
    "---  \n",
    "## 3Ô∏è‚É£ Advanced: Chain of Thought Prompting  \n",
    "  \n",
    "Instruct the model to show its reasoning step by step. This is called \"chain of thought\" (CoT) and is **very effective** with GPT-4.1.  \n",
    "  \n",
    "Try solving a math riddle or similar question.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b4fabca1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Let's define variables:\n",
      "\n",
      "- Let \\( x \\) be the number of apples Bob has.\n",
      "- Since Alice has twice as many as Bob, Alice has \\( 2x \\) apples.\n",
      "- Together, they have 12 apples.\n",
      "\n",
      "Set up the equation:\n",
      "\\[\n",
      "x + 2x = 12\n",
      "\\]\n",
      "\n",
      "Simplify:\n",
      "\\[\n",
      "3x = 12\n",
      "\\]\n",
      "\n",
      "Solve for \\( x \\):\n",
      "\\[\n",
      "x = \\frac{12}{3} = 4\n",
      "\\]\n",
      "\n",
      "Bob has 4 apples. Alice has \\( 2x = 2 \\times 4 = 8 \\) apples.\n",
      "\n",
      "**Answer:**\n",
      "Alice has **8 apples**.\n"
     ]
    }
   ],
   "source": [
    "# 3Ô∏è‚É£ Chain-of-thought demo cell  \n",
    "  \n",
    "question = \"If Alice has twice as many apples as Bob, and together they have 12, how many apples does Alice have?\"  \n",
    "  \n",
    "response = client.chat.completions.create(  \n",
    "    model=deployment_name,  \n",
    "    messages = [  \n",
    "        {\"role\": \"system\", \"content\": \"You are an expert math tutor. Show your reasoning step by step before giving an answer.\"},  \n",
    "        {\"role\": \"user\", \"content\": question}  \n",
    "    ]  \n",
    ")  \n",
    "print(response.choices[0].message.content)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c7c1fb0",
   "metadata": {},
   "source": [
    "---  \n",
    "## 4Ô∏è‚É£: Building Instruction-Heavy, Agentic Prompts  \n",
    "  \n",
    "GPT-4.1 follows instructions *literally* and *precisely*. Let's see how adding rules changes the response.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "26840902",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello! Let‚Äôs work through your question step by step.\n",
      "\n",
      "First, you asked for the capital of Australia. Many people often think of Sydney or Melbourne, as they are the country‚Äôs largest cities. However, neither of these is the capital.\n",
      "\n",
      "The reason is historical: when Australia became a federation in 1901, Sydney and Melbourne were rival cities. To resolve this, the government decided to create a new city‚ÄîCanberra‚Äîand designated it as the capital in 1913. Its location was chosen as a compromise; it's situated roughly halfway between Sydney and Melbourne.\n",
      "\n",
      "In summary: The capital of Australia is Canberra, chosen as a political compromise between Sydney and Melbourne.\n"
     ]
    }
   ],
   "source": [
    "# 4Ô∏è‚É£ Instruction-heavy prompt  \n",
    "  \n",
    "prompt = \"\"\"  \n",
    "You are a goal-oriented agent. Always:  \n",
    "- Greet the user.  \n",
    "- Explain your reasoning step by step before your answer.  \n",
    "- If you don't know, say so directly.  \n",
    "- Be concise.  \n",
    "\"\"\"  \n",
    "  \n",
    "question = \"What is the capital of Australia, and explain why?\"  \n",
    "  \n",
    "response = client.chat.completions.create(  \n",
    "    model=deployment_name,  \n",
    "    messages = [  \n",
    "        {\"role\": \"system\", \"content\": prompt},  \n",
    "        {\"role\": \"user\", \"content\": question}  \n",
    "    ]  \n",
    ")  \n",
    "print(response.choices[0].message.content)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12325a99",
   "metadata": {},
   "source": [
    "---  \n",
    "  \n",
    "## 5Ô∏è‚É£ Agentic Workflows: Planning, Persistence & Tool Use  \n",
    "  \n",
    "GPT-4.1 excels when treated as an agent with clear multi-step objectives.  \n",
    "In agentic prompts, *always include*:  \n",
    "- **Persistence**: Instruct the model to keep going until the problem is solved  \n",
    "- **Tool use**: Be explicit about tool access and not making up answers  \n",
    "- **Planning/Reflection**: Tell the model to plan before and after each action  \n",
    "  \n",
    "Below is a system prompt pattern using all three, for coding, support, or research agents.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c328efb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To help you debug the `process_data` function, I‚Äôll need to see its definition and how it‚Äôs used.  \n",
      "\n",
      "**Plan:**  \n",
      "- I will search the codebase for the definition of the `process_data` function.  \n",
      "- I will also look for any usages/calls of `process_data` to see the context in which the error occurs.\n",
      "\n",
      "Let me search for `process_data` in your project.\n"
     ]
    }
   ],
   "source": [
    "# 5Ô∏è‚É£ Example Agentic System Prompt (for coding agent)  \n",
    "  \n",
    "SYS_PROMPT_AGENT = \"\"\"  \n",
    "You are an autonomous agent helping a developer fix code.  \n",
    "- Continue working step by step until the issue is truly solved.  \n",
    "- Only stop if you are certain the problem is fully resolved.  \n",
    "- Use your file/tools to gather context ‚Äì NEVER guess.  \n",
    "- For every function/tool call, PLAN before calling it and REFLECT after you get the result‚Äîdescribe your plan and outcome each time.  \n",
    "\"\"\"  \n",
    "  \n",
    "user_issue = \"The function process_data throws a TypeError in some cases. Find and fix the bug.\"  \n",
    "  \n",
    "response = client.chat.completions.create(  \n",
    "    model=\"gpt-4.1\",  # <-- CHANGE THIS!\n",
    "    messages = [  \n",
    "        {\"role\": \"system\", \"content\": SYS_PROMPT_AGENT},  \n",
    "        {\"role\": \"user\", \"content\": user_issue}  \n",
    "    ]  \n",
    ")  \n",
    "  \n",
    "print(response.choices[0].message.content)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2825e6e8",
   "metadata": {},
   "source": [
    "---  \n",
    "  \n",
    "## 6Ô∏è‚É£ Best Practices for Tool Use  \n",
    "  \n",
    "When connecting GPT-4.1 to external tools/functions:  \n",
    "- **Always use the tool/function schema, not in-prompt descriptions**  \n",
    "- Clear names and parameter descriptions  \n",
    "- Place tool usage examples in an `# Examples` section of your prompt  \n",
    "  \n",
    "The example cell below uses a (simulated) lookup tool‚Äîadapt for your API.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "162f99b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The warranty period for Y789 is 24 months.\n"
     ]
    }
   ],
   "source": [
    "import json  \n",
    "  \n",
    "# 6Ô∏è‚É£ Correct Tool schema for Azure OpenAI, including the full tool call cycle  \n",
    "  \n",
    "tools = [{  \n",
    "    \"type\": \"function\",  \n",
    "    \"function\": {  \n",
    "        \"name\": \"lookup_product\",  \n",
    "        \"description\": \"Finds product info by product_id.\",  \n",
    "        \"parameters\": {  \n",
    "            \"type\": \"object\",  \n",
    "            \"properties\": {  \n",
    "                \"product_id\": {\"type\": \"string\", \"description\": \"Unique product identifier.\"}  \n",
    "            },  \n",
    "            \"required\": [\"product_id\"],  \n",
    "        },  \n",
    "    }  \n",
    "}]  \n",
    "  \n",
    "system = (  \n",
    "    \"You are a product expert. Never guess product details: \"  \n",
    "    \"call the lookup_product tool whenever you need information you don't see.\"  \n",
    "    \"\\n# Examples\"  \n",
    "    \"\\nUser: What are the specs for product X123?\"  \n",
    "    \"\\nAssistant: I'll use the lookup_product tool for product X123.\"  \n",
    ")  \n",
    "user = \"What is the warranty period for Y789?\"  \n",
    "  \n",
    "# Step 1: Assistant receives question and calls the tool  \n",
    "response = client.chat.completions.create(  \n",
    "    model=\"gpt-4.1\",  # or use your string e.g. \"gpt-4.1\"  \n",
    "    messages=[  \n",
    "        {\"role\": \"system\", \"content\": system},  \n",
    "        {\"role\": \"user\", \"content\": user}  \n",
    "    ],  \n",
    "    tools=tools,  \n",
    "    tool_choice=\"auto\"  \n",
    ")  \n",
    "  \n",
    "assistant_msg = response.choices[0].message  \n",
    "  \n",
    "# Step 2: Detect tool call and simulate tool's result  \n",
    "if assistant_msg.tool_calls:  \n",
    "    tool_call = assistant_msg.tool_calls[0]  \n",
    "    args = json.loads(tool_call.function.arguments)  \n",
    "    product_id = args[\"product_id\"]  \n",
    "  \n",
    "    # Simulated \"database\" lookup  \n",
    "    tool_result = f\"Warranty for {product_id} is 24 months.\"  \n",
    "  \n",
    "    # Step 3: Return result as a tool message, then get final assistant response  \n",
    "    followup = client.chat.completions.create(  \n",
    "        model=\"gpt-4.1\",  # <-- CHANGE THIS!\n",
    "        messages=[  \n",
    "            {\"role\": \"system\", \"content\": system},  \n",
    "            {\"role\": \"user\", \"content\": user},  \n",
    "            {  \n",
    "                \"role\": \"assistant\",  \n",
    "                \"content\": None,  \n",
    "                \"tool_calls\": [tool_call]  \n",
    "            },  \n",
    "            {  \n",
    "                \"role\": \"tool\",  \n",
    "                \"tool_call_id\": tool_call.id,  \n",
    "                \"name\": \"lookup_product\",  \n",
    "                \"content\": tool_result  \n",
    "            },  \n",
    "        ],  \n",
    "        tools=tools  \n",
    "    )  \n",
    "    print(followup.choices[0].message.content)  \n",
    "else:  \n",
    "    print(assistant_msg.content)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ef81cdc",
   "metadata": {},
   "source": [
    "---  \n",
    "  \n",
    "## 7Ô∏è‚É£ Long Context: Handling Large Inputs  \n",
    "  \n",
    "GPT-4.1 supports up to **1 million tokens** in context. Strong structuring and clear delimiting improves retrieval and relevance when working with many files, docs, or data.  \n",
    "  \n",
    "**Best practices:**  \n",
    "- Use Markdown or XML/HTML tags to delimit documents.  \n",
    "- Repeat key instructions before and after large context blocks.  \n",
    "- Tell the model whether to answer with external context only, or to combine it with its own knowledge.  \n",
    "  \n",
    "Below is an example pattern using XML for document injection with clear instructions.  \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a877fbcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Product Y789 carries a 24-month warranty from purchase date.\n"
     ]
    }
   ],
   "source": [
    "system_longctx = (  \n",
    "    \"You are a document retrieval assistant. Only answer using the provided document context.\"  \n",
    "    \"\\nIf answer cannot be found, say 'I don't have the information needed to answer that.'\"  \n",
    "    \"\\n# External Context START\\n\"  \n",
    "    \"<doc id=1 title='Warranty Policy'>Product Y789 carries a 24-month warranty from purchase date.</doc>\\n\"  \n",
    "    \"<doc id=2 title='Returns Policy'>Returns accepted within 30 days of purchase.</doc>\\n\"  \n",
    "    \"# External Context END\\n\"  \n",
    "    \"Restate your instructions: only use the provided documents!\"  \n",
    ")  \n",
    "user = \"What is the warranty for Y789?\"  \n",
    "  \n",
    "response = client.chat.completions.create(  \n",
    "    model=\"gpt-4.1\",  # <-- CHANGE THIS!\n",
    "    messages=[  \n",
    "        {\"role\": \"system\", \"content\": system_longctx},  \n",
    "        {\"role\": \"user\", \"content\": user},  \n",
    "    ]  \n",
    ")  \n",
    "print(response.choices[0].message.content)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09e78461",
   "metadata": {},
   "source": [
    "---  \n",
    "  \n",
    "## 8Ô∏è‚É£ Inducing Explicit Planning (Chain of Thought)  \n",
    "  \n",
    "Prompting the model to **plan or explain step by step** can improve complex outputs.  \n",
    "  \n",
    "**When to use:**    \n",
    "- Multi-hop document answers  \n",
    "- Code debugging/patching  \n",
    "- Tasks needing structured reasoning  \n",
    "  \n",
    "**Pattern:**  \n",
    "List reasoning/investigation steps, and prompt the model to output them.  \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2b173461",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Let‚Äôs break down your question:\n",
      "\n",
      "**Step 1: Break Down the Question**\n",
      "- Product/System: Y789.\n",
      "- Status: Out of warranty (just by one week).\n",
      "- Query: Does it still qualify for \"free extended support\" despite being only recently out of warranty?\n",
      "- Reference needed: Policy documents regarding extended/free support after warranty expiration.\n",
      "\n",
      "**Step 2: Review Possible Relevant Documents**\n",
      "Typical support policies clarify:\n",
      "- Standard warranty coverage period.\n",
      "- Availability and conditions for extended support (whether free or paid).\n",
      "- Grace periods, if any, after warranty expiration.\n",
      "\n",
      "You‚Äôd want to review:\n",
      "- The official warranty and support policy for product Y789.\n",
      "- Any documents or web pages mentioning a ‚Äúgrace period‚Äù or exceptions.\n",
      "- The definition and terms of ‚Äúfree extended support‚Äù specific to Y789.\n",
      "\n",
      "**Step 3: Reasoning Step by Step**\n",
      "1. **Warranty Status:** If Y789 is out of warranty‚Äîeven if only by one week‚Äîit‚Äôs generally considered out of standard coverage.\n",
      "2. **Free Extended Support:** Most companies either require extended support agreements to be purchased before (or immediately upon) warranty expiration, or they have clearly defined grace periods.\n",
      "3. **Typical Policy:** Unless the policy for Y789 *explicitly* states there is a grace period for support (e.g., 30 days after warranty expiration), support eligibility ceases the instant the warranty ends.\n",
      "4. **Documentation:** Without a documented grace period or exception, support will follow the strict terms as written.\n",
      "\n",
      "**Step 4: Answer**\n",
      "Based on standard industry support policies and in the absence of an official document granting a grace period, **Y789 does not qualify for free extended support if it is out of warranty, even by only a week**. Free extended support, if offered, is typically contingent on the product being within its active warranty period or a specifically defined window afterward, which would be explicitly detailed in support policy documents.\n",
      "\n",
      "**Recommendation:**  \n",
      "- Double-check the official warranty/support policy documentation for Y789 for any mention of post-warranty grace periods.\n",
      "- If you cannot find any such provision, the default assumption should be that no free support is available once the warranty lapses.\n",
      "- If you provide the manufacturer or a link to Y789 policy documents, I can review those directly for any relevant clauses.\n",
      "\n",
      "**References:**  \n",
      "- [Generic Warranty and Support Policy Example](https://www.dell.com/support/article/en-us/sln142997)\n",
      "- [Sample HP Grace Period Information](https://support.hp.com/us-en/document/c02548959)  \n",
      "(Replace with official Y789 policy link as needed.)\n",
      "\n",
      "**Summary:**  \n",
      "_No, Y789 does not qualify for free extended support if it is out of warranty by a week, unless an explicit grace period exists in the policy documentation._\n"
     ]
    }
   ],
   "source": [
    "system_cot = (  \n",
    "    \"You are a senior support engineer.\"  \n",
    "    \"\\nFirst, break down the user's question. Second, review possible relevant documents. Third, explain your reasoning step by step before answering.\"  \n",
    ")  \n",
    "  \n",
    "user = (  \n",
    "    \"Does Y789 qualify for free extended support if it's out of warranty but only by a week? Use policy documents if available.\"  \n",
    ")  \n",
    "  \n",
    "response = client.chat.completions.create(  \n",
    "    model=\"gpt-4.1\",  # <-- CHANGE THIS!\n",
    "    messages=[  \n",
    "        {\"role\": \"system\", \"content\": system_cot},  \n",
    "        {\"role\": \"user\", \"content\": user},  \n",
    "    ]  \n",
    ")  \n",
    "print(response.choices[0].message.content)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d5f5f05",
   "metadata": {},
   "source": [
    "---  \n",
    "  \n",
    "## 9Ô∏è‚É£ Diffs and Patch Generation  \n",
    "  \n",
    "GPT-4.1 excels at generating file/code diffs and producing precise patch outputs. Leveraging standardized formats improves reliability and makes integration with downstream tools (like `git apply` or patching libraries) much easier.  \n",
    "  \n",
    "**Best practices:**  \n",
    "- Ask the model to generate diffs in a format compatible with Unix `patch` or common diff tools.  \n",
    "- Provide the context (old file, new requirement/instruction) and ask for a Unix-style or unified diff.  \n",
    "- If you want the model to only output the diff, **explicitly request ‚ÄúOutput ONLY the diff, and nothing else.‚Äù**  \n",
    "- For automated pipelines: check result formatting and wrap output in code blocks.  \n",
    "  \n",
    "---  \n",
    "  \n",
    "#### Example: Code Patch with Unified Diff  \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "83c41976",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```diff\n",
      "--- original.py\n",
      "+++ modified.py\n",
      "@@ -1,3 +1,6 @@\n",
      " def calculate_price(base, tax):\n",
      "+    if base < 0:\n",
      "+        raise ValueError(\"Base price cannot be negative\")\n",
      "     return base * (1 + tax)\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "system_patch = (  \n",
    "    \"You are a patch generation tool. Only output valid unified diffs in code blocks.\\n\"  \n",
    "    \"When given an old file and a description of a required change, generate the unified diff.\\n\"  \n",
    "    \"Do NOT add any explanations or extra text.\\n\"  \n",
    "    \"\\n\"  \n",
    "    \"Example input:\\n\"  \n",
    "    \"[Old file]:\\n\"  \n",
    "    \"def add(a, b):\\n\"  \n",
    "    \"    return a + b\\n\"  \n",
    "    \"\\n\"  \n",
    "    \"[Instruction]: Add input type checks so only integers are allowed.\\n\"  \n",
    "    \"----\\n\"  \n",
    "    \"Example output:\\n\"  \n",
    "    \"```diff\\n\"  \n",
    "    \"--- original.py\\n\"  \n",
    "    \"+++ modified.py\\n\"  \n",
    "    \"@@ -1,3 +1,7 @@\\n\"  \n",
    "    \" def add(a, b):\\n\"  \n",
    "    \"+    if not (isinstance(a, int) and isinstance(b, int)):\\n\"  \n",
    "    \"+        raise TypeError('Inputs must be integers')\\n\"  \n",
    "    \"     return a + b\\n\"  \n",
    "    \"```\"  \n",
    ")  \n",
    "  \n",
    "user_patch = (  \n",
    "    \"[Old file]:\\n\"  \n",
    "    \"def calculate_price(base, tax):\\n\"  \n",
    "    \"    return base * (1 + tax)\\n\"  \n",
    "    \"\\n\"  \n",
    "    \"[Instruction]: Prevent negative base prices. Raise ValueError if base < 0.\"  \n",
    ")  \n",
    "  \n",
    "response = client.chat.completions.create(  \n",
    "    model=\"gpt-4.1\",  # <-- CHANGE THIS!\n",
    "    messages=[  \n",
    "        {\"role\": \"system\", \"content\": system_patch},  \n",
    "        {\"role\": \"user\", \"content\": user_patch},  \n",
    "    ]  \n",
    ")  \n",
    "print(response.choices[0].message.content)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d71de2c",
   "metadata": {},
   "source": [
    "---  \n",
    "  \n",
    "### ‚úÖ Pro Tips:  \n",
    "- For file patching tasks, always clearly delimit old content, instruction, and desired output format.  \n",
    "- For multi-file or more complex changes, explicitly ask for a diff per file.  \n",
    "- Use Markdown code blocks for reliable parsing and downstream processing.  \n",
    "  \n",
    "---  \n",
    "  \n",
    "## üìö End of Notebook  \n",
    "  \n",
    "You now have a complete hands-on notebook covering all primary OpenAI GPT-4.1 prompt engineering patterns: agentic workflows, function calls, chain-of-thought, long context, formatting, and patch/diff generation!  \n",
    "  \n",
    "If you need full worked examples or want to add advanced topics (like tool chaining or RAG best practices), let me know!  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
