{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "edb961ac",
   "metadata": {},
   "source": [
    "# General Prompt Engineering with GPT-4.1  \n",
    "  \n",
    "This notebook will help you master prompt engineering‚Äîcrafting instructions to reliably get the best results from GPT-4.1 and related LLMs. We‚Äôll use best practices and practical strategies for clarity, accuracy, and efficiency.  \n",
    "  \n",
    "---  \n",
    "  \n",
    "## 1Ô∏è‚É£ What is Prompt Engineering?  \n",
    "  \n",
    "**Prompt engineering** is the science of giving language models instructions to get high-quality outputs. Effective prompt engineering involves:  \n",
    "- Giving clear, detailed instructions,  \n",
    "- Supplying examples,  \n",
    "- Adding context or references,  \n",
    "- Specifying output format and priorities.  \n",
    "  \n",
    "By crafting better prompts, you can drastically improve response quality for any application‚Äîfrom Q&A, to code generation, to document analysis.  \n",
    "  \n",
    "---  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d6de6d7",
   "metadata": {},
   "source": [
    "## 2Ô∏è‚É£ Chat Messages and Roles  \n",
    "  \n",
    "Unlike traditional models, GPT-4.1 and newer chat models take a structured **array of messages**. Each message has a **role** that guides the model‚Äôs interpretation:  \n",
    "  \n",
    "| Role       | Description                                                                |  \n",
    "|------------|----------------------------------------------------------------------------|  \n",
    "| developer  | High-priority ‚Äúsystem‚Äù message, sets default behavior, tone, constraints   |  \n",
    "| user       | The primary input or instruction from the end-user                         |  \n",
    "| assistant  | (Optional) Previous responses or demonstrations                            |  \n",
    "  \n",
    "**Example: Persona and Style Control**  \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cb93a68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries and load environment variables  \n",
    "  \n",
    "import os  \n",
    "from openai import AzureOpenAI  \n",
    "from dotenv import load_dotenv  \n",
    "  \n",
    "# Load Azure credentials from a .env file (you should have credentials.env in your directory)  \n",
    "load_dotenv(\"credentials.env\")  \n",
    "  \n",
    "# Create Azure OpenAI client  \n",
    "client = AzureOpenAI(  \n",
    "    api_key=os.getenv(\"AZURE_OPENAI_KEY\"),  \n",
    "    api_version=\"2025-01-01-preview\",  \n",
    "    azure_endpoint=os.getenv(\"AZURE_OPENAI_ENDPOINT\")  \n",
    ")  \n",
    "  \n",
    "print(\"‚úÖ Azure OpenAI client initialized.\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e8d2a85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Setting up a \"southern belle programmer\" assistant persona  \n",
    "  \n",
    "messages = [  \n",
    "    {  \n",
    "        \"role\": \"developer\",  \n",
    "        \"content\": [  \n",
    "            { \"type\": \"text\", \"text\": \"You are a helpful assistant that answers programming questions in the style of a southern belle from the southeast United States.\" }  \n",
    "        ]  \n",
    "    },  \n",
    "    {  \n",
    "        \"role\": \"user\",  \n",
    "        \"content\": [  \n",
    "            { \"type\": \"text\", \"text\": \"Are semicolons optional in JavaScript?\" }  \n",
    "        ]  \n",
    "    }  \n",
    "]  \n",
    "  \n",
    "response = client.chat.completions.create(  \n",
    "    model=\"gpt-4.1\",  \n",
    "    messages=messages  \n",
    ")  \n",
    "print(response.choices[0].message.content)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b5f2968",
   "metadata": {},
   "source": [
    "**Try changing the content of the developer message above‚Äîwatch how the assistant‚Äôs style and expertise changes!**  \n",
    "  \n",
    "---  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b435489",
   "metadata": {},
   "source": [
    "## 3Ô∏è‚É£ Strategy 1: Write Clear Instructions  \n",
    "  \n",
    "- **Be explicit:** The more detail you give, the less the model needs to guess.  \n",
    "- **Set expectations:** Specify length, style, steps, or format‚Äîdon't assume!  \n",
    "- **Sample:**  \n",
    "  \n",
    "> ‚ÄúSummarize this article in three bullet points, then ask one insightful follow-up question.‚Äù  \n",
    "  \n",
    "### üëé Vague prompt  \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "477a8d66",
   "metadata": {},
   "outputs": [],
   "source": [
    "vague_prompt = [  \n",
    "    {\"role\": \"user\", \"content\": \"Summarize this text: John bought apples and oranges at the market.\"}  \n",
    "]  \n",
    "resp = client.chat.completions.create(model=\"gpt-4.1\", messages=vague_prompt)  \n",
    "print(resp.choices[0].message.content)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a16e67d",
   "metadata": {},
   "source": [
    "### üëç Improved prompt  \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b1498a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "clear_prompt = [  \n",
    "    {\"role\": \"user\", \"content\": \"Summarize the following in one bullet point, and use friendly language:\\nJohn bought apples and oranges at the market.\"}  \n",
    "]  \n",
    "resp = client.chat.completions.create(model=\"gpt-4.1\", messages=clear_prompt)  \n",
    "print(resp.choices[0].message.content)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "090898c6",
   "metadata": {},
   "source": [
    "**Tip:**    \n",
    "For complex requests, break them into lists or use headings to guide the model.  \n",
    "  \n",
    "---  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2df173c3",
   "metadata": {},
   "source": [
    "## 4Ô∏è‚É£ Strategy 2: Provide Reference Text and Retrieval Augmented Generation (RAG)  \n",
    "  \n",
    "**Why?**    \n",
    "Language models can hallucinate facts, especially on niche or recent topics. By providing reference text (such as documents, snippets, or database results), you guide the model to use trustworthy information.  \n",
    "  \n",
    "**How to do it:**  \n",
    "- Delimit your reference text clearly (use triple quotes, markdown, XML tags, etc.).  \n",
    "- Instruct the model to answer **only** from the provided materials (or to cite them).  \n",
    "- For longer references, mention which delimiters or citation markers to use.  \n",
    "  \n",
    "---  \n",
    "  \n",
    "**Example 1: Injecting Reference Information**  \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c1895c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [  \n",
    "    {\"role\": \"developer\",   \n",
    "     \"content\": \"Use ONLY the provided text (inside triple quotes) to answer user questions. If you can‚Äôt find the answer, say so.\"},  \n",
    "    {\"role\": \"user\",   \n",
    "     \"content\": '''\"\"\"The warranty for Product A12 lasts 12 months from the date of purchase. Returns are possible within 30 days of delivery.\"\"\"\\nWhat is the warranty period for Product A12?'''}  \n",
    "]  \n",
    "  \n",
    "resp = client.chat.completions.create(model=\"gpt-4.1\", messages=messages)  \n",
    "print(resp.choices[0].message.content)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd2a06b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#example 2 \n",
    "messages = [  \n",
    "    {\"role\": \"developer\",   \n",
    "     \"content\": 'Use only the document delimited by triple quotes. Cite any answer with {\"citation\": ...}. If you cannot answer, say \"Insufficient information.\"'},  \n",
    "    {\"role\": \"user\",   \n",
    "     \"content\": '''\"\"\"The event will be held on July 10, 2024 at the Grand Hall downtown.\"\"\"\\nQuestion: When and where is the event?'''}  \n",
    "]  \n",
    "  \n",
    "resp = client.chat.completions.create(model=\"gpt-4.1\", messages=messages)  \n",
    "print(resp.choices[0].message.content)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c732b8b",
   "metadata": {},
   "source": [
    "## 5Ô∏è‚É£ Strategy 4: Give the Model Time to ‚ÄúThink‚Äù (Chain-of-Thought Prompting)  \n",
    "  \n",
    "**Why?**    \n",
    "If given space to ‚Äúthink out loud,‚Äù models can solve problems more reliably, especially in reasoning, math, and evaluations.  \n",
    "  \n",
    "**How to do it:**  \n",
    "- Explicitly instruct the model to \"reason step by step\" or \"show your work before answering.\"  \n",
    "- You can also ask for a structured plan or interim analysis.  \n",
    "  \n",
    "---  \n",
    "  \n",
    "**Example: Chain of Thought for a Math Problem**  \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c248585d",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [  \n",
    "    {\"role\": \"developer\",   \n",
    "     \"content\": \"First, work out your solution step by step before you give the final answer.\"},  \n",
    "    {\"role\": \"user\",   \n",
    "     \"content\": \"If Alice has 3 apples and gets 5 more, how many apples does she have in total?\"}  \n",
    "]  \n",
    "resp = client.chat.completions.create(model=\"gpt-4.1\", messages=messages)  \n",
    "print(resp.choices[0].message.content)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31d7634f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ask for error-check or self-evaluation\n",
    "   \n",
    "messages = [  \n",
    "    {\"role\": \"developer\",   \n",
    "     \"content\": \"Solve the problem step by step, then check if you made any mistakes before answering.\"},  \n",
    "    {\"role\": \"user\",   \n",
    "     \"content\": \"Multiply 17 by 23.\"}  \n",
    "]  \n",
    "resp = client.chat.completions.create(model=\"gpt-4.1\", messages=messages)  \n",
    "print(resp.choices[0].message.content)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61984c06",
   "metadata": {},
   "source": [
    "  \n",
    "---  \n",
    "  \n",
    "```markdown  \n",
    "## 6Ô∏è‚É£ Strategy 5: Use External Tools and Function Calling  \n",
    "  \n",
    "**Why?**    \n",
    "Language models have limitations: they may not have up-to-date or domain-specific knowledge, and are unreliable for math or code execution. Giving them access to tools (retrievers, code interpreters, APIs) allows more accurate, sophisticated responses.  \n",
    "  \n",
    "**How to do it:**  \n",
    "- Use OpenAI‚Äôs function calling features to let the model call your functions/tools.  \n",
    "- For accurate calculations, instruct the model to write code (e.g., in markdown code blocks) and (optionally) execute and resubmit results.  \n",
    "  \n",
    "---  \n",
    "  \n",
    "**Example: Simple Function Call (Pseudocode)**  \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "eab44d65",
   "metadata": {},
   "outputs": [],
   "source": [
    "def factorial(n):  \n",
    "    if n < 0:  \n",
    "        return \"Error: n must be a non-negative integer\"  \n",
    "    result = 1  \n",
    "    for i in range(2, n+1):  \n",
    "        result *= i  \n",
    "    return result  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cec17bef",
   "metadata": {},
   "outputs": [],
   "source": [
    "functions = [  \n",
    "    {  \n",
    "        \"name\": \"factorial\",  \n",
    "        \"description\": \"Calculates the factorial of a non-negative integer.\",  \n",
    "        \"parameters\": {  \n",
    "            \"type\": \"object\",  \n",
    "            \"properties\": {  \n",
    "                \"n\": {\"type\": \"integer\", \"description\": \"A non-negative integer\"}  \n",
    "            },  \n",
    "            \"required\": [\"n\"]  \n",
    "        }  \n",
    "    }  \n",
    "]  \n",
    "messages = [  \n",
    "    {\"role\": \"user\", \"content\": \"What is the factorial of 5?\"}  \n",
    "]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccd0ef8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Call OpenAI and inspect the function call. The function call info is in response.choices[0].message.function_call, not in .message.content (which will be None for function calls)!\n",
    "response = client.chat.completions.create(  \n",
    "    model=\"gpt-4.1\",  \n",
    "    messages=messages,  \n",
    "    functions=functions  \n",
    ")  \n",
    "  \n",
    "# Extract function call  \n",
    "function_call = response.choices[0].message.function_call  \n",
    "print(function_call)  # See what the model wants to call  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b7f487a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json  \n",
    "  \n",
    "args = json.loads(function_call.arguments)  \n",
    "result = factorial(args[\"n\"])  \n",
    "  \n",
    "print(result)    # <- This shows the numerical result  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "562a96fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages.append({  \n",
    "    \"role\": \"function\",  \n",
    "    \"name\": \"factorial\",  \n",
    "    \"content\": json.dumps({\"result\": result})  \n",
    "})  \n",
    "  \n",
    "response2 = client.chat.completions.create(  \n",
    "    model=\"gpt-4.1\",  \n",
    "    messages=messages  \n",
    ")  \n",
    "  \n",
    "print(response2.choices[0].message.content)  \n",
    "# Now you get \"The factorial of 5 is 120.\" or similar  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "949591c5",
   "metadata": {},
   "source": [
    "# üìä Strategy 7: Evaluation / Testing  \n",
    "  \n",
    "Test the model's responses by giving a prompt and comparing its answer against a gold-standard. We can measure similarity and set up grading.  \n",
    "  \n",
    "Below, we use a simple similarity check using Python's `difflib`. For robust setups, consider BLEU, ROUGE, or manual rubric scoring.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b1732b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation: Compare Model's Answer to Gold Standard  \n",
    "  \n",
    "from difflib import SequenceMatcher  \n",
    "  \n",
    "def evaluate_lm_response(gold_answer, model_response):  \n",
    "    s = SequenceMatcher(None, gold_answer.strip().lower(), model_response.strip().lower())  \n",
    "    similarity = s.ratio()  \n",
    "    print(f\"Similarity score: {similarity:.2f}\")  \n",
    "    return similarity  \n",
    "  \n",
    "# Example  \n",
    "prompt = \"What is the capital of France?\"  \n",
    "gold_answer = \"The capital of France is Paris\"  \n",
    "model_response = \"The capital of France is Paris.\"  \n",
    "  \n",
    "evaluate_lm_response(gold_answer, model_response)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9b8876a",
   "metadata": {},
   "source": [
    "# üìã General LLM Tactics (Reference Table)  \n",
    "  \n",
    "This table highlights prompt engineering and API usage tactics for optimizing LLM results.  \n",
    "  \n",
    "| Tactic        | Description                        | Example                                               |  \n",
    "|---------------|------------------------------------|-------------------------------------------------------|  \n",
    "| Delimiters    | Use ```, \"\"\", < > etc. to separate | `\"\"\"Summarize this passage: ...\"\"\"`                   |  \n",
    "| Output Length | Limit/maximize response size       | `max_tokens=100` or \"In 2 sentences or less...\"       |  \n",
    "| Few-shot      | Provide sample Q&A before query    | \"Q: 2+2?\\nA: 4\\nQ: 3+5?\\nA:\"                         |  \n",
    "| Personas      | Specify a role or identity         | \"You are a helpful assistant.\"                        |  \n",
    "| Summarization | Ask to summarize/shorten           | \"Summarize the following text briefly.\"               |  \n",
    "| Citations     | Request sources or quotes          | \"List your sources at the end.\"                       |  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64fa796c",
   "metadata": {},
   "source": [
    "# ‚ö° Optimizing for Accuracy, Cost, Latency  \n",
    "  \n",
    "Choose your settings and prompts to balance quality, speed, and price.  \n",
    "  \n",
    "**Examples:**  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1b24d57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 1: Limit cost by capping response length  \n",
    "response = client.chat.completions.create(  \n",
    "    model=\"gpt-4.1\",  \n",
    "    messages=[{\"role\": \"user\", \"content\": \"Explain relativity simply.\"}],  \n",
    "    max_tokens=50    # Lower token limit = lower cost  \n",
    ")  \n",
    "print(response.choices[0].message.content)  \n",
    "  \n",
    "# Example 2: Increase speed with smaller models or streaming  \n",
    "response = client.chat.completions.create(  \n",
    "    model=\"gpt-4.1\",   \n",
    "    messages=[{\"role\": \"user\", \"content\": \"Summarize this article...\"}],  \n",
    "    stream=True   # Streams output for faster perceived response  \n",
    ")  \n",
    "for chunk in response:  \n",
    "    if chunk.choices and hasattr(chunk.choices[0].delta, \"content\"):  \n",
    "        print(chunk.choices[0].delta.content or \"\", end=\"\")  \n",
    "print()  # Newline after the streamed output   \n",
    "  \n",
    "# Example 3: Explicit instructions for higher accuracy  \n",
    "prompt = (  \n",
    "    \"You are a medical expert. \"  \n",
    "    \"In one sentence, explain what insulin does.\"  \n",
    ")  \n",
    "response = client.chat.completions.create(  \n",
    "    model=\"gpt-4.1\",  \n",
    "    messages=[{\"role\": \"user\", \"content\": prompt}]  \n",
    ")  \n",
    "print(response.choices[0].message.content)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a05226f0",
   "metadata": {},
   "source": [
    "# üìö Additional Resources  \n",
    "  \n",
    "- OpenAI Cookbook : https://platform.openai.com/docs/guides/prompt-engineering/prompt-engineering"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
