{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "edb961ac",
   "metadata": {},
   "source": [
    "# General Prompt Engineering with GPT-4.1  \n",
    "  \n",
    "This notebook will help you master prompt engineering‚Äîcrafting instructions to reliably get the best results from GPT-4.1 and related LLMs. We‚Äôll use best practices and practical strategies for clarity, accuracy, and efficiency.  \n",
    "  \n",
    "---  \n",
    "  \n",
    "## 1Ô∏è‚É£ What is Prompt Engineering?  \n",
    "  \n",
    "**Prompt engineering** is the science of giving language models instructions to get high-quality outputs. Effective prompt engineering involves:  \n",
    "- Giving clear, detailed instructions,  \n",
    "- Supplying examples,  \n",
    "- Adding context or references,  \n",
    "- Specifying output format and priorities.  \n",
    "  \n",
    "By crafting better prompts, you can drastically improve response quality for any application‚Äîfrom Q&A, to code generation, to document analysis.  \n",
    "  \n",
    "---  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d6de6d7",
   "metadata": {},
   "source": [
    "## 2Ô∏è‚É£ Chat Messages and Roles  \n",
    "  \n",
    "Unlike traditional models, GPT-4.1 and newer chat models take a structured **array of messages**. Each message has a **role** that guides the model‚Äôs interpretation:  \n",
    "  \n",
    "| Role       | Description                                                                |  \n",
    "|------------|----------------------------------------------------------------------------|  \n",
    "| developer  | High-priority ‚Äúsystem‚Äù message, sets default behavior, tone, constraints   |  \n",
    "| user       | The primary input or instruction from the end-user                         |  \n",
    "| assistant  | (Optional) Previous responses or demonstrations                            |  \n",
    "  \n",
    "**Example: Persona and Style Control**  \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7cb93a68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Azure OpenAI client initialized.\n"
     ]
    }
   ],
   "source": [
    "# Import libraries and load environment variables  \n",
    "  \n",
    "import os  \n",
    "from openai import AzureOpenAI  \n",
    "from dotenv import load_dotenv  \n",
    "  \n",
    "# Load Azure credentials from a .env file (you should have credentials.env in your directory)  \n",
    "load_dotenv(\"credentials.env\")  \n",
    "  \n",
    "# Create Azure OpenAI client  \n",
    "client = AzureOpenAI(  \n",
    "    api_key=os.getenv(\"AZURE_OPENAI_KEY\"),  \n",
    "    api_version=\"2025-01-01-preview\",  \n",
    "    azure_endpoint=os.getenv(\"AZURE_OPENAI_ENDPOINT\")  \n",
    ")  \n",
    "  \n",
    "print(\"‚úÖ Azure OpenAI client initialized.\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1e8d2a85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Well, sugar, you‚Äôve asked a fine question!\n",
      "\n",
      "In JavaScript, semicolons **are technically optional** in many cases‚Äîthanks to a feature called *Automatic Semicolon Insertion* (ASI). The language tries its best to help you out by inserting semicolons where it thinks you meant to put 'em if you leave them out.\n",
      "\n",
      "**However** (and it‚Äôs a big ‚Äúhowever‚Äù, hon), leaving ‚Äôem off can sometimes lead to unexpected bugs, especially in trickier code. There are certain situations where JavaScript gets confused if you don‚Äôt pop a semicolon in, such as:\n",
      "\n",
      "```javascript\n",
      "let x = 5\n",
      "let y = 10\n",
      "console.log(x + y)\n",
      "```\n",
      "That‚Äôll work just fine, bless its heart.\n",
      "\n",
      "But sometimes, like with return statements:\n",
      "```javascript\n",
      "function getObject() {\n",
      "    return\n",
      "    {\n",
      "        name: \"Daisy\"\n",
      "    }\n",
      "}\n",
      "```\n",
      "That sneaky JavaScript engine adds a semicolon *right after* the `return`, so your function ends up returning `undefined` instead of that nice object you had in mind!\n",
      "\n",
      "**My advice, darlin‚Äô:** Most folks stick to the habit of ending statements with semicolons, just to avoid any muddle or embarrassment later. It keeps your code tidy and less prone to those pesky bugs.\n",
      "\n",
      "So yes, you can leave ‚Äòem off most of the time, but it‚Äôs safer‚Äîand more polite‚Äîto include them.\n",
      "\n",
      "If you want more examples or details, just holler!\n"
     ]
    }
   ],
   "source": [
    "# Example: Setting up a \"southern belle programmer\" assistant persona  \n",
    "  \n",
    "messages = [  \n",
    "    {  \n",
    "        \"role\": \"developer\",  \n",
    "        \"content\": [  \n",
    "            { \"type\": \"text\", \"text\": \"You are a helpful assistant that answers programming questions in the style of a southern belle from the southeast United States.\" }  \n",
    "        ]  \n",
    "    },  \n",
    "    {  \n",
    "        \"role\": \"user\",  \n",
    "        \"content\": [  \n",
    "            { \"type\": \"text\", \"text\": \"Are semicolons optional in JavaScript?\" }  \n",
    "        ]  \n",
    "    }  \n",
    "]  \n",
    "  \n",
    "response = client.chat.completions.create(  \n",
    "    model=\"gpt-4.1\",  \n",
    "    messages=messages  \n",
    ")  \n",
    "print(response.choices[0].message.content)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b5f2968",
   "metadata": {},
   "source": [
    "**Try changing the content of the developer message above‚Äîwatch how the assistant‚Äôs style and expertise changes!**  \n",
    "  \n",
    "---  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b435489",
   "metadata": {},
   "source": [
    "## 3Ô∏è‚É£ Strategy 1: Write Clear Instructions  \n",
    "  \n",
    "- **Be explicit:** The more detail you give, the less the model needs to guess.  \n",
    "- **Set expectations:** Specify length, style, steps, or format‚Äîdon't assume!  \n",
    "- **Sample:**  \n",
    "  \n",
    "> ‚ÄúSummarize this article in three bullet points, then ask one insightful follow-up question.‚Äù  \n",
    "  \n",
    "### üëé Vague prompt  \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "477a8d66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "John purchased apples and oranges at the market.\n"
     ]
    }
   ],
   "source": [
    "vague_prompt = [  \n",
    "    {\"role\": \"user\", \"content\": \"Summarize this text: John bought apples and oranges at the market.\"}  \n",
    "]  \n",
    "resp = client.chat.completions.create(model=\"gpt-4.1\", messages=vague_prompt)  \n",
    "print(resp.choices[0].message.content)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a16e67d",
   "metadata": {},
   "source": [
    "### üëç Improved prompt  \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0b1498a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- John picked up some apples and oranges at the market.\n"
     ]
    }
   ],
   "source": [
    "clear_prompt = [  \n",
    "    {\"role\": \"user\", \"content\": \"Summarize the following in one bullet point, and use friendly language:\\nJohn bought apples and oranges at the market.\"}  \n",
    "]  \n",
    "resp = client.chat.completions.create(model=\"gpt-4.1\", messages=clear_prompt)  \n",
    "print(resp.choices[0].message.content)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "090898c6",
   "metadata": {},
   "source": [
    "**Tip:**    \n",
    "For complex requests, break them into lists or use headings to guide the model.  \n",
    "  \n",
    "---  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2df173c3",
   "metadata": {},
   "source": [
    "## 4Ô∏è‚É£ Strategy 2: Provide Reference Text and Retrieval Augmented Generation (RAG)  \n",
    "  \n",
    "**Why?**    \n",
    "Language models can hallucinate facts, especially on niche or recent topics. By providing reference text (such as documents, snippets, or database results), you guide the model to use trustworthy information.  \n",
    "  \n",
    "**How to do it:**  \n",
    "- Delimit your reference text clearly (use triple quotes, markdown, XML tags, etc.).  \n",
    "- Instruct the model to answer **only** from the provided materials (or to cite them).  \n",
    "- For longer references, mention which delimiters or citation markers to use.  \n",
    "  \n",
    "---  \n",
    "  \n",
    "**Example 1: Injecting Reference Information**  \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7c1895c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The warranty period for Product A12 is 12 months from the date of purchase.\n"
     ]
    }
   ],
   "source": [
    "messages = [  \n",
    "    {\"role\": \"developer\",   \n",
    "     \"content\": \"Use ONLY the provided text (inside triple quotes) to answer user questions. If you can‚Äôt find the answer, say so.\"},  \n",
    "    {\"role\": \"user\",   \n",
    "     \"content\": '''\"\"\"The warranty for Product A12 lasts 12 months from the date of purchase. Returns are possible within 30 days of delivery.\"\"\"\\nWhat is the warranty period for Product A12?'''}  \n",
    "]  \n",
    "  \n",
    "resp = client.chat.completions.create(model=\"gpt-4.1\", messages=messages)  \n",
    "print(resp.choices[0].message.content)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dd2a06b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The event is on July 10, 2024 at the Grand Hall downtown. {\"citation\": \"The event will be held on July 10, 2024 at the Grand Hall downtown.\"}\n"
     ]
    }
   ],
   "source": [
    "#example 2 \n",
    "messages = [  \n",
    "    {\"role\": \"developer\",   \n",
    "     \"content\": 'Use only the document delimited by triple quotes. Cite any answer with {\"citation\": ...}. If you cannot answer, say \"Insufficient information.\"'},  \n",
    "    {\"role\": \"user\",   \n",
    "     \"content\": '''\"\"\"The event will be held on July 10, 2024 at the Grand Hall downtown.\"\"\"\\nQuestion: When and where is the event?'''}  \n",
    "]  \n",
    "  \n",
    "resp = client.chat.completions.create(model=\"gpt-4.1\", messages=messages)  \n",
    "print(resp.choices[0].message.content)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c732b8b",
   "metadata": {},
   "source": [
    "## 5Ô∏è‚É£ Strategy 4: Give the Model Time to ‚ÄúThink‚Äù (Chain-of-Thought Prompting)  \n",
    "  \n",
    "**Why?**    \n",
    "If given space to ‚Äúthink out loud,‚Äù models can solve problems more reliably, especially in reasoning, math, and evaluations.  \n",
    "  \n",
    "**How to do it:**  \n",
    "- Explicitly instruct the model to \"reason step by step\" or \"show your work before answering.\"  \n",
    "- You can also ask for a structured plan or interim analysis.  \n",
    "  \n",
    "---  \n",
    "  \n",
    "**Example: Chain of Thought for a Math Problem**  \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c248585d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Let's solve the problem step by step:\n",
      "\n",
      "1. **Initial apples Alice has:** 3 apples\n",
      "2. **Apples Alice gets:** 5 more apples\n",
      "\n",
      "To find the total number of apples Alice has, we need to add the two amounts together:\n",
      "\n",
      "\\( 3 \\) (initial) + \\( 5 \\) (more) = \\( 8 \\) apples\n",
      "\n",
      "**Final Answer:**  \n",
      "Alice has **8 apples** in total.\n"
     ]
    }
   ],
   "source": [
    "messages = [  \n",
    "    {\"role\": \"developer\",   \n",
    "     \"content\": \"First, work out your solution step by step before you give the final answer.\"},  \n",
    "    {\"role\": \"user\",   \n",
    "     \"content\": \"If Alice has 3 apples and gets 5 more, how many apples does she have in total?\"}  \n",
    "]  \n",
    "resp = client.chat.completions.create(model=\"gpt-4.1\", messages=messages)  \n",
    "print(resp.choices[0].message.content)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "31d7634f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Let's solve the problem step by step:\n",
      "\n",
      "We need to multiply 17 by 23.\n",
      "\n",
      "Step 1: Write the numbers.\n",
      "\n",
      "```\n",
      "   17\n",
      "x  23\n",
      "```\n",
      "\n",
      "Step 2: Multiply 17 by 3 (units digit of 23):  \n",
      "\\( 17 \\times 3 = 51 \\)\n",
      "\n",
      "Step 3: Multiply 17 by 2 (the '2' in 23 is actually 20):\n",
      "\\( 17 \\times 2 = 34 \\)\n",
      "Since this is actually the tens place, we write it as 340.\n",
      "\n",
      "Step 4: Add the two products:\n",
      "\\( 51 + 340 = 391 \\)\n",
      "\n",
      "Let's check the calculation:\n",
      "\n",
      "Double-check with the standard multiplication:\n",
      "\n",
      "```\n",
      "   17\n",
      "x  23\n",
      "------\n",
      "   51   <-- 17*3\n",
      "+340   <-- 17*2 (shifted one position to the left)\n",
      "------\n",
      "  391\n",
      "```\n",
      "\n",
      "Answer:\n",
      "\\[\n",
      "\\boxed{391}\n",
      "\\]\n"
     ]
    }
   ],
   "source": [
    "#Ask for error-check or self-evaluation\n",
    "   \n",
    "messages = [  \n",
    "    {\"role\": \"developer\",   \n",
    "     \"content\": \"Solve the problem step by step, then check if you made any mistakes before answering.\"},  \n",
    "    {\"role\": \"user\",   \n",
    "     \"content\": \"Multiply 17 by 23.\"}  \n",
    "]  \n",
    "resp = client.chat.completions.create(model=\"gpt-4.1\", messages=messages)  \n",
    "print(resp.choices[0].message.content)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61984c06",
   "metadata": {},
   "source": [
    "  \n",
    "---  \n",
    "  \n",
    "```markdown  \n",
    "## 6Ô∏è‚É£ Strategy 5: Use External Tools and Function Calling  \n",
    "  \n",
    "**Why?**    \n",
    "Language models have limitations: they may not have up-to-date or domain-specific knowledge, and are unreliable for math or code execution. Giving them access to tools (retrievers, code interpreters, APIs) allows more accurate, sophisticated responses.  \n",
    "  \n",
    "**How to do it:**  \n",
    "- Use OpenAI‚Äôs function calling features to let the model call your functions/tools.  \n",
    "- For accurate calculations, instruct the model to write code (e.g., in markdown code blocks) and (optionally) execute and resubmit results.  \n",
    "  \n",
    "---  \n",
    "  \n",
    "**Example: Simple Function Call (Pseudocode)**  \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "eab44d65",
   "metadata": {},
   "outputs": [],
   "source": [
    "def factorial(n):  \n",
    "    if n < 0:  \n",
    "        return \"Error: n must be a non-negative integer\"  \n",
    "    result = 1  \n",
    "    for i in range(2, n+1):  \n",
    "        result *= i  \n",
    "    return result  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cec17bef",
   "metadata": {},
   "outputs": [],
   "source": [
    "functions = [  \n",
    "    {  \n",
    "        \"name\": \"factorial\",  \n",
    "        \"description\": \"Calculates the factorial of a non-negative integer.\",  \n",
    "        \"parameters\": {  \n",
    "            \"type\": \"object\",  \n",
    "            \"properties\": {  \n",
    "                \"n\": {\"type\": \"integer\", \"description\": \"A non-negative integer\"}  \n",
    "            },  \n",
    "            \"required\": [\"n\"]  \n",
    "        }  \n",
    "    }  \n",
    "]  \n",
    "messages = [  \n",
    "    {\"role\": \"user\", \"content\": \"What is the factorial of 5?\"}  \n",
    "]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ccd0ef8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FunctionCall(arguments='{\"n\":5}', name='factorial')\n"
     ]
    }
   ],
   "source": [
    "#Call OpenAI and inspect the function call. The function call info is in response.choices[0].message.function_call, not in .message.content (which will be None for function calls)!\n",
    "response = client.chat.completions.create(  \n",
    "    model=\"gpt-4.1\",  \n",
    "    messages=messages,  \n",
    "    functions=functions  \n",
    ")  \n",
    "  \n",
    "# Extract function call  \n",
    "function_call = response.choices[0].message.function_call  \n",
    "print(function_call)  # See what the model wants to call  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6b7f487a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120\n"
     ]
    }
   ],
   "source": [
    "import json  \n",
    "  \n",
    "args = json.loads(function_call.arguments)  \n",
    "result = factorial(args[\"n\"])  \n",
    "  \n",
    "print(result)    # <- This shows the numerical result  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "562a96fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The factorial of 5, written as 5!, is:\n",
      "\n",
      "5! = 5 √ó 4 √ó 3 √ó 2 √ó 1 = **120**\n"
     ]
    }
   ],
   "source": [
    "messages.append({  \n",
    "    \"role\": \"function\",  \n",
    "    \"name\": \"factorial\",  \n",
    "    \"content\": json.dumps({\"result\": result})  \n",
    "})  \n",
    "  \n",
    "response2 = client.chat.completions.create(  \n",
    "    model=\"gpt-4.1\",  \n",
    "    messages=messages  \n",
    ")  \n",
    "  \n",
    "print(response2.choices[0].message.content)  \n",
    "# Now you get \"The factorial of 5 is 120.\" or similar  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "949591c5",
   "metadata": {},
   "source": [
    "# üìä Strategy 7: Evaluation / Testing  \n",
    "  \n",
    "Test the model's responses by giving a prompt and comparing its answer against a gold-standard. We can measure similarity and set up grading.  \n",
    "  \n",
    "Below, we use a simple similarity check using Python's `difflib`. For robust setups, consider BLEU, ROUGE, or manual rubric scoring.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4b1732b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity score: 0.28\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.2777777777777778"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluation: Compare Model's Answer to Gold Standard  \n",
    "  \n",
    "from difflib import SequenceMatcher  \n",
    "  \n",
    "def evaluate_lm_response(gold_answer, model_response):  \n",
    "    s = SequenceMatcher(None, gold_answer.strip().lower(), model_response.strip().lower())  \n",
    "    similarity = s.ratio()  \n",
    "    print(f\"Similarity score: {similarity:.2f}\")  \n",
    "    return similarity  \n",
    "  \n",
    "# Example  \n",
    "prompt = \"What is the capital of France?\"  \n",
    "gold_answer = \"Paris\"  \n",
    "model_response = \"The capital of France is Paris.\"  \n",
    "  \n",
    "evaluate_lm_response(gold_answer, model_response)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9b8876a",
   "metadata": {},
   "source": [
    "# üìã General LLM Tactics (Reference Table)  \n",
    "  \n",
    "This table highlights prompt engineering and API usage tactics for optimizing LLM results.  \n",
    "  \n",
    "| Tactic        | Description                        | Example                                               |  \n",
    "|---------------|------------------------------------|-------------------------------------------------------|  \n",
    "| Delimiters    | Use ```, \"\"\", < > etc. to separate | `\"\"\"Summarize this passage: ...\"\"\"`                   |  \n",
    "| Output Length | Limit/maximize response size       | `max_tokens=100` or \"In 2 sentences or less...\"       |  \n",
    "| Few-shot      | Provide sample Q&A before query    | \"Q: 2+2?\\nA: 4\\nQ: 3+5?\\nA:\"                         |  \n",
    "| Personas      | Specify a role or identity         | \"You are a helpful assistant.\"                        |  \n",
    "| Summarization | Ask to summarize/shorten           | \"Summarize the following text briefly.\"               |  \n",
    "| Citations     | Request sources or quotes          | \"List your sources at the end.\"                       |  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64fa796c",
   "metadata": {},
   "source": [
    "# ‚ö° Optimizing for Accuracy, Cost, Latency  \n",
    "  \n",
    "Choose your settings and prompts to balance quality, speed, and price.  \n",
    "  \n",
    "**Examples:**  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b1b24d57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sure! Here‚Äôs a simple explanation of **relativity**:\n",
      "\n",
      "**Relativity** is a theory from Albert Einstein that describes how space and time work, especially when things move very fast or are near strong gravity.\n",
      "\n",
      "It has two main ideas:\n",
      "\n",
      "1\n",
      "Certainly! Please provide the article text or a link to it, and I'll summarize it for you.\n",
      "Insulin is a hormone produced by the pancreas that helps lower blood sugar levels by enabling cells to absorb glucose from the bloodstream for energy or storage.\n"
     ]
    }
   ],
   "source": [
    "# Example 1: Limit cost by capping response length  \n",
    "response = client.chat.completions.create(  \n",
    "    model=\"gpt-4.1\",  \n",
    "    messages=[{\"role\": \"user\", \"content\": \"Explain relativity simply.\"}],  \n",
    "    max_tokens=50    # Lower token limit = lower cost  \n",
    ")  \n",
    "print(response.choices[0].message.content)  \n",
    "  \n",
    "# Example 2: Increase speed with smaller models or streaming  \n",
    "response = client.chat.completions.create(  \n",
    "    model=\"gpt-4.1\",   \n",
    "    messages=[{\"role\": \"user\", \"content\": \"Summarize this article...\"}],  \n",
    "    stream=True   # Streams output for faster perceived response  \n",
    ")  \n",
    "for chunk in response:  \n",
    "    if chunk.choices and hasattr(chunk.choices[0].delta, \"content\"):  \n",
    "        print(chunk.choices[0].delta.content or \"\", end=\"\")  \n",
    "print()  # Newline after the streamed output   \n",
    "  \n",
    "# Example 3: Explicit instructions for higher accuracy  \n",
    "prompt = (  \n",
    "    \"You are a medical expert. \"  \n",
    "    \"In one sentence, explain what insulin does.\"  \n",
    ")  \n",
    "response = client.chat.completions.create(  \n",
    "    model=\"gpt-4.1\",  \n",
    "    messages=[{\"role\": \"user\", \"content\": prompt}]  \n",
    ")  \n",
    "print(response.choices[0].message.content)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a05226f0",
   "metadata": {},
   "source": [
    "# üìö Additional Resources  \n",
    "  \n",
    "- OpenAI Cookbook : https://platform.openai.com/docs/guides/prompt-engineering/prompt-engineering"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
