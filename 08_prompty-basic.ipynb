{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Getting started with prompty"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 0. Install dependent packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "gather": {
          "logged": 1720091586128
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Note: you may need to restart the kernel to use updated packages.Collecting typing-extensions==4.8\n",
            "  Downloading typing_extensions-4.8.0-py3-none-any.whl (31 kB)\n",
            "Installing collected packages: typing-extensions\n",
            "  Attempting uninstall: typing-extensions\n",
            "    Found existing installation: typing_extensions 4.13.2\n",
            "    Uninstalling typing_extensions-4.13.2:\n",
            "      Successfully uninstalled typing_extensions-4.13.2\n",
            "Successfully installed typing-extensions-4.8.0\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "typing-inspection 0.4.0 requires typing-extensions>=4.12.0, but you have typing-extensions 4.8.0 which is incompatible.\n",
            "pydantic 2.11.3 requires typing-extensions>=4.12.2, but you have typing-extensions 4.8.0 which is incompatible.\n",
            "openai 1.73.0 requires typing-extensions<5,>=4.11, but you have typing-extensions 4.8.0 which is incompatible.\n"
          ]
        }
      ],
      "source": [
        "%pip install --force-reinstall typing-extensions==4.8"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "#%%capture --no-stderr # magic command in Jupyter notebooks is used to capture the output of a cell,\n",
        "#%pip install prompflow-core"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Execute a Prompty\n",
        "\n",
        "Prompty is a file with .prompty extension for developing prompt template. \n",
        "The prompty asset is a markdown file with a modified front matter. \n",
        "The front matter is in yaml format that contains a number of metadata fields which defines model configuration and expected inputs of the prompty."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "gather": {
          "logged": 1724222847390
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "---\n",
            "name: Basic Prompt\n",
            "description: A basic prompt that uses the chat API to answer questions\n",
            "model:\n",
            "    api: chat\n",
            "    configuration:\n",
            "        type: azure_openai\n",
            "        azure_deployment: gpt-4-0125-Preview\n",
            "    parameters:\n",
            "        max_tokens: 128\n",
            "        temperature: 0.2\n",
            "inputs:\n",
            "  question:\n",
            "    type: string\n",
            "sample:\n",
            "  \"question\": \"Who is the most famous person in the world?\"\n",
            "---\n",
            "system:\n",
            "You are an AI assistant who helps people find information.\n",
            "As the assistant, you answer questions briefly, succinctly. \n",
            "\n",
            "user:\n",
            "{{question}}\n"
          ]
        }
      ],
      "source": [
        "with open(\"prompty/basic.prompty\") as fin:\n",
        "    print(fin.read())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "gather": {
          "logged": 1724223950474
        }
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import os\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "load_dotenv(\"credentials.env\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "gather": {
          "logged": 1724224827422
        }
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'The capital of France is Paris.'"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from promptflow.core import Prompty\n",
        "\n",
        "# load prompty as a flow\n",
        "f = Prompty.load(source=\"prompty/basic.prompty\")\n",
        "\n",
        "# execute the flow as function\n",
        "result = f(question=\"What is the capital of France?\")\n",
        "result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "You can override configuration with `AzureOpenAIModelConfiguration` and `OpenAIModelConfiguration`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "gather": {
          "logged": 1724223963289
        }
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'The capital of France is Paris.'"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from promptflow.core import AzureOpenAIModelConfiguration, OpenAIModelConfiguration\n",
        "\n",
        "# override configuration with AzureOpenAIModelConfiguration\n",
        "configuration = AzureOpenAIModelConfiguration(\n",
        "    #azure_endpoint=\"${env:AZURE_OPENAI_ENDPOINT}\",  # Use ${env:<ENV_NAME>} to surround the environment variable name.\n",
        "    #api_key=\"${env:AZURE_OPENAI_API_KEY}\",\n",
        "    azure_deployment=\"gpt-4o\",\n",
        ")\n",
        "\n",
        "# override configuration with OpenAIModelConfiguration\n",
        "# configuration = OpenAIModelConfiguration(\n",
        "#     base_url=\"${env:OPENAI_BASE_URL}\",\n",
        "#     api_key=\"${env:OPENAI_API_KEY}\",\n",
        "#     model=\"gpt-3.5-turbo\"\n",
        "# )\n",
        "\n",
        "override_model = {\"configuration\": configuration, \"parameters\": {\"max_tokens\": 512}}\n",
        "\n",
        "# load prompty as a flow\n",
        "f = Prompty.load(source=\"prompty/basic.prompty\", model=override_model)\n",
        "\n",
        "# execute the flow as function\n",
        "result = f(question=\"What is the capital of France?\")\n",
        "result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Visualize trace by using start_trace"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "gather": {
          "logged": 1724223476284
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting prompt flow service...\n",
            "Start prompt flow service on 127.0.0.1:23333, version: 1.13.0.\n",
            "You can stop the prompt flow service with the following command:'\u001b[1mpf service stop\u001b[0m'.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from promptflow.tracing import start_trace\n",
        "\n",
        "# start a trace session, and print a url for user to check trace\n",
        "start_trace()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Re-run below cell will collect a trace in trace UI."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "gather": {
          "logged": 1724223504326
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "You can view the trace detail from the following URL:\n",
            "http://127.0.0.1:23333/v1.0/ui/traces/?#collection=AzureOpenAI_Advanced&uiTraceId=0x940d19a845a3aed6165fb02aa414bd8f\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "'Tokyo.'"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# rerun the function, which will be recorded in the trace\n",
        "question = \"What is the capital of Japan?\"\n",
        "ground_truth = \"Tokyo\"\n",
        "result = f(question=question)\n",
        "result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Eval the result \n",
        "\n",
        "Note: the eval flow returns a `json_object`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "gather": {
          "logged": 1724223736306
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "You can view the trace detail from the following URL:\n",
            "http://127.0.0.1:23333/v1.0/ui/traces/?#collection=AzureOpenAI_Advanced&uiTraceId=0xd5fd8d891da39648c15201cb9cff5c1c\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'score': '5', 'explanation': 'Tokyo is indeed the capital of Japan.'}"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# load prompty as a flow\n",
        "eval_flow = Prompty.load(\"prompty/eval.prompty\")\n",
        "# execute the flow as function\n",
        "result = eval_flow(question=question, ground_truth=ground_truth, answer=result)\n",
        "result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "gather": {
          "logged": 1724223769332
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Prompt flow service stop on 127.0.0.1:23333.\r\n"
          ]
        }
      ],
      "source": [
        "!pf service stop"
      ]
    }
  ],
  "metadata": {
    "build_doc": {
      "author": [
        "lalala123123@github.com",
        "wangchao1230@github.com"
      ],
      "category": "local",
      "section": "Prompty",
      "weight": 10
    },
    "description": "A quickstart tutorial to run a prompty and evaluate it.",
    "kernel_info": {
      "name": "python310-sdkv2"
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    },
    "microsoft": {
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      },
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      }
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    },
    "resources": "examples/requirements.txt, examples/prompty/basic, examples/prompty/eval-basic"
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
