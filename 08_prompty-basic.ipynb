{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Getting started with prompty"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 0. Install dependent packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1720091586128
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "%pip install --force-reinstall typing-extensions==4.8"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "#%%capture --no-stderr # magic command in Jupyter notebooks is used to capture the output of a cell,\n",
        "#%pip install prompflow-core"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Execute a Prompty\n",
        "\n",
        "Prompty is a file with .prompty extension for developing prompt template. \n",
        "The prompty asset is a markdown file with a modified front matter. \n",
        "The front matter is in yaml format that contains a number of metadata fields which defines model configuration and expected inputs of the prompty."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1724222847390
        }
      },
      "outputs": [],
      "source": [
        "with open(\"prompty/basic.prompty\") as fin:\n",
        "    print(fin.read())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "gather": {
          "logged": 1724223950474
        }
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import os\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "load_dotenv(\"credentials.env\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1724224827422
        }
      },
      "outputs": [],
      "source": [
        "from promptflow.core import Prompty\n",
        "\n",
        "# load prompty as a flow\n",
        "f = Prompty.load(source=\"prompty/basic.prompty\")\n",
        "\n",
        "# execute the flow as function\n",
        "result = f(question=\"What is the capital of France?\")\n",
        "result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "You can override configuration with `AzureOpenAIModelConfiguration` and `OpenAIModelConfiguration`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1724223963289
        }
      },
      "outputs": [],
      "source": [
        "from promptflow.core import AzureOpenAIModelConfiguration, OpenAIModelConfiguration\n",
        "\n",
        "# override configuration with AzureOpenAIModelConfiguration\n",
        "configuration = AzureOpenAIModelConfiguration(\n",
        "    #azure_endpoint=\"${env:AZURE_OPENAI_ENDPOINT}\",  # Use ${env:<ENV_NAME>} to surround the environment variable name.\n",
        "    #api_key=\"${env:AZURE_OPENAI_API_KEY}\",\n",
        "    azure_deployment=\"gpt-4o\",\n",
        ")\n",
        "\n",
        "# override configuration with OpenAIModelConfiguration\n",
        "# configuration = OpenAIModelConfiguration(\n",
        "#     base_url=\"${env:OPENAI_BASE_URL}\",\n",
        "#     api_key=\"${env:OPENAI_API_KEY}\",\n",
        "#     model=\"gpt-3.5-turbo\"\n",
        "# )\n",
        "\n",
        "override_model = {\"configuration\": configuration, \"parameters\": {\"max_tokens\": 512}}\n",
        "\n",
        "# load prompty as a flow\n",
        "f = Prompty.load(source=\"prompty/basic.prompty\", model=override_model)\n",
        "\n",
        "# execute the flow as function\n",
        "result = f(question=\"can you tell me about Microsoft\")\n",
        "result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Visualize trace by using start_trace"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1724223476284
        }
      },
      "outputs": [],
      "source": [
        "from promptflow.tracing import start_trace\n",
        "\n",
        "# start a trace session, and print a url for user to check trace\n",
        "start_trace()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Re-run below cell will collect a trace in trace UI."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1724223504326
        }
      },
      "outputs": [],
      "source": [
        "# rerun the function, which will be recorded in the trace\n",
        "question = \"What is the capital of Japan?\"\n",
        "ground_truth = \"Tokyo\"\n",
        "result = f(question=question)\n",
        "result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Eval the result \n",
        "\n",
        "Note: the eval flow returns a `json_object`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1724223736306
        }
      },
      "outputs": [],
      "source": [
        "# load prompty as a flow\n",
        "eval_flow = Prompty.load(\"prompty/eval.prompty\")\n",
        "# execute the flow as function\n",
        "result = eval_flow(question=question, ground_truth=ground_truth, answer=result)\n",
        "result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1724223769332
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "!pf service stop"
      ]
    }
  ],
  "metadata": {
    "build_doc": {
      "author": [
        "lalala123123@github.com",
        "wangchao1230@github.com"
      ],
      "category": "local",
      "section": "Prompty",
      "weight": 10
    },
    "description": "A quickstart tutorial to run a prompty and evaluate it.",
    "kernel_info": {
      "name": "python310-sdkv2"
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    },
    "microsoft": {
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      },
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      }
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    },
    "resources": "examples/requirements.txt, examples/prompty/basic, examples/prompty/eval-basic"
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
