{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Note: Below is only for Managed Identity:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "gather": {
          "logged": 1720090341386
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# import openai\n",
        "# from openai import AzureOpenAI\n",
        "# import os \n",
        "# from azure.identity import ManagedIdentityCredential\n",
        "\n",
        "# default_credential=ManagedIdentityCredential(client_id=\"XXX\")\n",
        "# token=default_credential.get_token(\"https://cognitiveservices.azure.com/.default\")\n",
        "# Resource_endpoint=\"XXXX\"\n",
        "\n",
        "# client = AzureOpenAI(\n",
        "#   azure_endpoint = Resource_endpoint, \n",
        "#   api_key=token.token,  \n",
        "#   api_version=\"2023-05-15\"\n",
        "# )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Install libraries:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pip install python-dotenv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pip install openai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "gather": {
          "logged": 1724165993950
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import openai\n",
        "from openai import AzureOpenAI\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "# Set up Azure OpenAI\n",
        "load_dotenv(\"credentials.env\")\n",
        "\n",
        "openai.api_type = \"azure\"\n",
        "    \n",
        "client = AzureOpenAI(\n",
        "    api_key=os.getenv(\"AZURE_OPENAI_API_KEY\"),  \n",
        "    api_version=\"2025-01-01-preview\", #latest GA API version: https://learn.microsoft.com/en-us/azure/ai-services/openai/api-version-deprecation\n",
        "    azure_endpoint = os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "gather": {
          "logged": 1724165995179
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "#print(os.getenv(\"AZURE_OPENAI_ENDPOINT\"))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Temperature\n",
        "\n",
        "Defaults to 1, Optional\n",
        "\n",
        "What sampling temperature to use, between 0 and 2. Higher values means the model will take more risks. Try 0.9 for more creative applications, and 0 for ones with a well-defined answer.\n",
        "\n",
        "We generally recommend altering this or top_p but not both."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "gather": {
          "logged": 1724166025018
        }
      },
      "outputs": [],
      "source": [
        "def call_openai(num_times, start_phrase, temperature):\n",
        "    for i in range(num_times):\n",
        "        deployment_name = \"gpt-4o\"  # Use the correct model identifier\n",
        "\n",
        "        # Construct the conversation messages\n",
        "        messages = [\n",
        "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "            {\"role\": \"user\", \"content\": start_phrase}\n",
        "        ]\n",
        "\n",
        "        # Send a chat completion request\n",
        "        response = client.chat.completions.create(\n",
        "            model=deployment_name,\n",
        "            messages=messages,\n",
        "            temperature=temperature,\n",
        "            max_tokens=10\n",
        "        )\n",
        "\n",
        "        # Print the generated response content\n",
        "        print(response.choices[0].message.content.strip())\n",
        "        print(\"*****************************\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1724166034947
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "call_openai(5, 'Azure machine learning is ', temperature = 0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1724166042947
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "call_openai(5, 'Azure machine learning is ', temperature =0.5)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Top_p\n",
        "\n",
        "Defaults to 1, Optional\n",
        "\n",
        "top_p parameter which stands for “top probability” and an alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered. \n",
        "\n",
        "The top_p refers to the probability mass that should be used when considering the next word in the generated text. Essentially it ** sets a threshold for the probability of the next word being chosen and only considers the most likely words that exceed that threshold.**\n",
        "\n",
        "We generally recommend altering this or temperature but not both."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "def call_openai(num_times, start_phrase, top_p):\n",
        "    for i in range(num_times):\n",
        "        deployment_name = \"gpt-4o\"  # Use the correct model identifier\n",
        "\n",
        "        # Construct the conversation messages\n",
        "        messages = [\n",
        "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "            {\"role\": \"user\", \"content\": start_phrase}\n",
        "        ]\n",
        "\n",
        "        # Send a chat completion request\n",
        "        response = client.chat.completions.create(\n",
        "            model=deployment_name,\n",
        "            messages=messages,\n",
        "            top_p=top_p,\n",
        "            max_tokens=30\n",
        "        )\n",
        "\n",
        "        # Print the generated response content\n",
        "        print(response.choices[0].message.content.strip())\n",
        "        print(\"*****************************\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1724167575058
        }
      },
      "outputs": [],
      "source": [
        "call_openai(10, 'Azure machine learning is ', top_p = 0.1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1724167580916
        }
      },
      "outputs": [],
      "source": [
        "call_openai(10, 'Azure machine learning is ', top_p = 1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Max_Tokens \n",
        "\n",
        "Default value=16, Optional\n",
        "\n",
        "The maximum number of tokens to generate in the completion. **The token count of your prompt plus max_tokens can't exceed the model's context length. **"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1724167582913
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "deployment_name = \"gpt-4o\"  # This corresponds to the custom name of your deployed model.\n",
        "start_phrase = \"Azure machine learning is \"\n",
        "\n",
        "response = client.chat.completions.create(\n",
        "    model=deployment_name,\n",
        "    messages=[\n",
        "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "        {\"role\": \"user\", \"content\": start_phrase}\n",
        "    ],\n",
        "    temperature=1,\n",
        "    max_tokens=105\n",
        ")\n",
        "\n",
        "print(response.model_dump_json(indent=2))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1724167593915
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "start_phrase = \"Howden Group Holdings insurance \"\n",
        "\n",
        "response = client.chat.completions.create(\n",
        "    model=deployment_name,  # This corresponds to your custom deployment name\n",
        "    messages=[\n",
        "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "        {\"role\": \"user\", \"content\": start_phrase}\n",
        "    ],\n",
        "    temperature=1,\n",
        "    max_tokens=90\n",
        ")\n",
        "\n",
        "print(response.model_dump_json(indent=2))\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# n\n",
        "\n",
        "Defaults to 1, optional\n",
        "\n",
        "How many completions to generate for each prompt. To generate multiple completions, we specify the n request parameter, which simply stands for ** “number of completions” **\n",
        "\n",
        "Note: Because this parameter generates many completions, it can quickly consume your token quota. Use carefully and ensure that you have reasonable settings for max_tokens and stop."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1724220628328
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "deployment_name = \"gpt-4o\"  # This is your custom deployment name\n",
        "\n",
        "# Define the start phrase\n",
        "start_phrase = \"Azure machine learning is \"\n",
        "\n",
        "# Send a chat completion call to generate answers (3 completions)\n",
        "response = client.chat.completions.create(\n",
        "    model=deployment_name,\n",
        "    messages=[\n",
        "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "        {\"role\": \"user\", \"content\": start_phrase}\n",
        "    ],\n",
        "    temperature=0.7,\n",
        "    n=3  # Number of completions to generate\n",
        ")\n",
        "\n",
        "# Print each completion\n",
        "for choice in response.choices:\n",
        "    print(choice.message.content.strip())\n",
        "    print(\"**************************\")\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# logprobs\n",
        "\n",
        "Defaults to null, optional\n",
        "\n",
        "Log probabilities of output tokens indicate **the likelihood of each token occurring in the sequence given the context.** To simplify, a logprob is log(p), where p = probability of a token occurring at a specific position based on the previous tokens in the context.\n",
        "\n",
        "For example, if logprobs is 5, the API will return a list of the 5 most likely tokens. The API will always return the logprob of the sampled token, so there may be up to logprobs+1 elements in the response.\n",
        "\n",
        "** tokens ** — is an array of tokens generated by the language model. Each token is a word or part of a word.\n",
        "\n",
        "** token_logprobs ** — represents an array of log probabilities for each token in the tokens array. Log probability indicates the likelihood of the language model generating that token for the given prompt. The logprob values are negative, where smaller (more negative) numbers indicate a less likely outcome.\n",
        "\n",
        "** top_logprobs ** — represents an array of log probability objects, representing tokens most likely to be used for the completion. For example, if we specify the request parameter top_p = 0.5, then top_logprobs would contain log probabilities for top 50% of generated tokens."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1724220633357
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "deployment_name = \"gpt-4o\"  # Your custom deployment name\n",
        "\n",
        "start_phrase = \"Azure machine learning is \"\n",
        "\n",
        "response = client.chat.completions.create(\n",
        "    model=deployment_name,\n",
        "    messages=[\n",
        "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "        {\"role\": \"user\", \"content\": start_phrase}\n",
        "    ],\n",
        "    temperature=0,\n",
        "    n=1,\n",
        "    logprobs=True  # Correct boolean value\n",
        ")\n",
        "\n",
        "print(response.choices[0].message.content.strip())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "gather": {
          "logged": 1724220635388
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "#response.choices[0].text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1724220639371
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "print(response.model_dump_json(indent=2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pip install numpy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "gather": {
          "logged": 1724220643394
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1724220644380
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "print(\"Word values :\",response.choices[0].logprobs.tokens)\n",
        "print(\"Token log probabilities :\",response.choices[0].logprobs.token_logprobs)\n",
        "print(\"Top log linear probabilities :\",np.round(np.exp(response.choices[0].logprobs.token_logprobs)*100,2))\n",
        "\n",
        "print(\"Response:\", response.choices[0].text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Presence_penalty\n",
        "\n",
        "Defaults to 0, Optional -> 0 means there is really no penalty or reward for the same token appearing multiple times. \n",
        "\n",
        "Number between -2.0 and 2.0. Positive values penalize new tokens based on ** whether they appear in the text so far **, increasing the model's likelihood to talk about new topics.\n",
        "\n",
        "Smaller values (minimum -2) decrease the penalty and increase the chances of a token appearing, while higher values (maximum 2) increase the penalty and decrease the chances of a token appearing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "deployment_name = \"gpt-4o\"  # Your Azure OpenAI deployment name\n",
        "\n",
        "start_phrase = \"Azure machine learning is \"\n",
        "\n",
        "response = client.chat.completions.create(\n",
        "    model=deployment_name,\n",
        "    messages=[\n",
        "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "        {\"role\": \"user\", \"content\": start_phrase}\n",
        "    ],\n",
        "    presence_penalty=2,\n",
        "    max_tokens=50  # Use boolean, but may not return token-level info\n",
        ")\n",
        "\n",
        "# Output the response\n",
        "print(\"Response:\", response.choices[0].message.content.strip())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "deployment_name = \"gpt-4o\"  # Your Azure OpenAI deployment name\n",
        "\n",
        "start_phrase = \"Azure machine learning is \"\n",
        "\n",
        "response = client.chat.completions.create(\n",
        "    model=deployment_name,\n",
        "    messages=[\n",
        "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "        {\"role\": \"user\", \"content\": start_phrase}\n",
        "    ],\n",
        "    presence_penalty=-2,\n",
        "    max_tokens=50  # Use boolean, but may not return token-level info\n",
        ")\n",
        "\n",
        "# Output the response\n",
        "print(\"Response:\", response.choices[0].message.content.strip())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1724220660400
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "print(response.model_dump_json(indent=2))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "\n",
        "# Frequency_penalty\n",
        "\n",
        "Defaults to 0\n",
        "\n",
        "Number between -2.0 and 2.0. Positive values ** penalize new tokens based on their existing frequency in the text so far **, decreasing the model's likelihood to repeat the same line verbatim."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "# Best_of\n",
        "\n",
        "Note: Works with Completion API not Chat_completion API. \n",
        " Defaults to 1,optional\n",
        "\n",
        "This parameter tells the language model to generate multiple completions and return the best one, which is the one with the highest log probability per token.\n",
        "\n",
        "\n",
        "Note: Because this parameter generates many completions, it can quickly consume your token quota. Use carefully and ensure that you have reasonable settings for max_tokens and stop.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# deployment_name = \"gpt-4o\"  # Your Azure OpenAI deployment name  \n",
        "  \n",
        "# # Define the start phrase  \n",
        "# start_phrase = \"Azure OpenAI is \"  \n",
        "  \n",
        "# # Correct method call is `client.chat.completions.create`  \n",
        "# response = client.chat.completions.create(  \n",
        "#     model=deployment_name,   \n",
        "#     messages=[  \n",
        "#         {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},  \n",
        "#         {\"role\": \"user\", \"content\": start_phrase}  \n",
        "#     ],  \n",
        "#     max_tokens=50,  \n",
        "#     temperature=0.7\n",
        "#     #best_of=3\n",
        "      \n",
        "# )  \n",
        "  \n",
        "# # Output the best completion  \n",
        "# print(\"Response:\", response.choices[0].message.content.strip())  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# logit_bias\n",
        "\n",
        "Defaults to null\n",
        "\n",
        "The logit_bias request parameter is used to modify the likelihood of specified tokens appearing in the completion. We can use this parameter to provide hints to the language model about **which tokens we want or don’t want to appear in the completion**. It basically allows us to make the model more biased towards certain keywords or topics."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#-100 bias, which should completely prevent them from appearing.\n",
        "# 100 bias will show only that word\n",
        "\n",
        "deployment_name = \"gpt-4o\"  # Your Azure OpenAI deployment name  \n",
        "  \n",
        "# Define the start phrase  \n",
        "start_phrase = \"Azure OpenAI is \"  \n",
        "  \n",
        "# Correct method call is `client.chat.completions.create`  \n",
        "response = client.chat.completions.create(  \n",
        "    model=deployment_name,   \n",
        "    messages=[  \n",
        "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},  \n",
        "        {\"role\": \"user\", \"content\": start_phrase}  \n",
        "    ],  \n",
        "    max_tokens=50,  \n",
        "    logit_bias={\"30\":10, \"5936\": 0}\n",
        "      \n",
        ")  \n",
        "  \n",
        "# Output the best completion  \n",
        "print(\"Response:\", response.choices[0].message.content.strip())  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Echo and Stop"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "Note: The echo parameter only works with Completion API. \n",
        "\n",
        "By setting the echo parameter to true, you’re asking the language model to **return the prompt embedded within the completion.** This is useful for debugging."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "deployment_name = \"gpt-4o\"  # Your Azure OpenAI deployment name  \n",
        "  \n",
        "# Define the start phrase  \n",
        "start_phrase = \"Azure OpenAI is \"  \n",
        "  \n",
        "# Correct method call is `client.chat.completions.create`  \n",
        "response = client.chat.completions.create(  \n",
        "    model=deployment_name,   \n",
        "    messages=[  \n",
        "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},  \n",
        "        {\"role\": \"user\", \"content\": start_phrase}  \n",
        "    ],  \n",
        "    max_tokens=50,  \n",
        "    #echo=False\n",
        "      \n",
        ")  \n",
        "  \n",
        "# Output the best completion  \n",
        "print(\"Response:\", response.choices[0].message.content.strip())  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "deployment_name = \"gpt-4o\"  # Your Azure OpenAI deployment name  \n",
        "  \n",
        "# Define the start phrase  \n",
        "start_phrase = \"Azure OpenAI is \"  \n",
        "  \n",
        "# Correct method call is `client.chat.completions.create`  \n",
        "response = client.chat.completions.create(  \n",
        "    model=deployment_name,   \n",
        "    messages=[  \n",
        "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},  \n",
        "        {\"role\": \"user\", \"content\": start_phrase}  \n",
        "    ],  \n",
        "    max_tokens=50,\n",
        "    temperature=0,\n",
        "    stop=\"Microsoft\"  \n",
        "      \n",
        ")  \n",
        "  \n",
        "# Output the best completion  \n",
        "print(\"Response:\", response.choices[0].message.content.strip())  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "The ** stop ** parameter allows you to specify up to 4 sequences of text on which the language model will halt and return the result. This is useful for specifying early termination triggers for the language model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "deployment_name = \"gpt-4o\"  # Your Azure OpenAI deployment name  \n",
        "  \n",
        "# Define the start phrase  \n",
        "start_phrase = \"Azure OpenAI is \"  \n",
        "  \n",
        "# Correct method call is `client.chat.completions.create`  \n",
        "response = client.chat.completions.create(  \n",
        "    model=deployment_name,   \n",
        "    messages=[  \n",
        "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},  \n",
        "        {\"role\": \"user\", \"content\": start_phrase}  \n",
        "    ],  \n",
        "    max_tokens=50,\n",
        "    temperature=0,\n",
        "    stop=[\"Microsoft\",\"since\"]  \n",
        "      \n",
        ")  \n",
        "  \n",
        "# Output the best completion  \n",
        "print(\"Response:\", response.choices[0].message.content.strip())  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "https://learn.microsoft.com/en-us/azure/ai-services/openai/reference"
      ]
    }
  ],
  "metadata": {
    "kernel_info": {
      "name": "python310-sdkv2"
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    },
    "microsoft": {
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      },
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      }
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
