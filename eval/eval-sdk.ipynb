{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3973276c",
   "metadata": {},
   "source": [
    "1. Introduction\n",
    " \n",
    "The Azure AI Evaluation SDK allows you to quantitatively and qualitatively evaluate Generative AI applications both locally and at scale. It includes a variety of built-in evaluators you can use with your test data, and supports evaluation for both single-turn and multi-turn conversations, as well as multi-modal data (e.g., images)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89eda68f",
   "metadata": {},
   "source": [
    "2. Environment Setup\n",
    " \n",
    "Make sure you have access to the necessary Azure OpenAI resources. Set the following environment variables in your system (or in your notebook for demonstration):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa04cf80",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os  \n",
    "  \n",
    "os.environ[\"AZURE_ENDPOINT\"] = \"endpoint\"  \n",
    "os.environ[\"AZURE_API_KEY\"] = \"your keys\"  \n",
    "os.environ[\"AZURE_DEPLOYMENT_NAME\"] = \"model deployment name\"  \n",
    "os.environ[\"AZURE_API_VERSION\"] = \"api version\"   # e.g., \"2024-02-15-preview\"  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe6462d9",
   "metadata": {},
   "source": [
    "3. SDK Installation\n",
    " \n",
    "Install the Azure AI Evaluation SDK:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fad3fb7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install azure-ai-evaluation  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a63d492",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install azure-ai-projects"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69567c7e",
   "metadata": {},
   "source": [
    "4. Model Configuration\n",
    " \n",
    "Required for AI-assisted evaluators (except some safety evaluators):\n",
    "You need to specify which GPT model will be used as the judge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a6abef70",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.evaluation import AzureOpenAIModelConfiguration  \n",
    "  \n",
    "model_config = AzureOpenAIModelConfiguration(  \n",
    "    azure_endpoint=os.environ[\"AZURE_ENDPOINT\"],  \n",
    "    api_key=os.environ[\"AZURE_API_KEY\"],  \n",
    "    azure_deployment=os.environ[\"AZURE_DEPLOYMENT_NAME\"],  \n",
    "    api_version=os.environ[\"AZURE_API_VERSION\"],  \n",
    ")  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23df5c97",
   "metadata": {},
   "source": [
    "5. Running Built-in Evaluators (Single Row)\n",
    " \n",
    "Let's run an evaluator on a simple query-response pair using the RelevanceEvaluator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "201c5eca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.evaluation import RelevanceEvaluator  \n",
    "  \n",
    "query = \"What is the capital of France?\"  \n",
    "response = \"Paris.\"  \n",
    "  \n",
    "relevance_eval = RelevanceEvaluator(model_config)  \n",
    "result = relevance_eval(query=query, response=response)  \n",
    "print(result)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1102faf3",
   "metadata": {},
   "source": [
    "Supported Built-in Evaluators\n",
    "\n",
    "General purpose: CoherenceEvaluator, FluencyEvaluator, QAEvaluator, etc.\n",
    "Similarity: SimilarityEvaluator, F1ScoreEvaluator, BleuScoreEvaluator,...\n",
    "RAG: GroundednessEvaluator, RetrievalEvaluator, etc.\n",
    "Safety: ViolenceEvaluator, ContentSafetyEvaluator, ...\n",
    "See full list in Azure Docs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f29640f",
   "metadata": {},
   "source": [
    "6. Batch Evaluation with .jsonl Dataset\n",
    " \n",
    "Prepare your dataset as a .jsonl file (JSON Lines):\n",
    "\n",
    "Example: data.jsonl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "061f4757",
   "metadata": {},
   "source": [
    "{\"query\": \"What is the capital of France?\", \"response\": \"Paris.\"}  \n",
    "{\"query\": \"What atoms compose water?\", \"response\": \"Hydrogen and oxygen.\"}  \n",
    "{\"query\": \"What color is my shirt?\", \"response\": \"Blue.\"}  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "255286ae",
   "metadata": {},
   "source": [
    "You can now run evaluators over this dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0f74a18",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.evaluation import evaluate, GroundednessEvaluator  \n",
    "  \n",
    "groundedness_eval = GroundednessEvaluator(model_config)  \n",
    "  \n",
    "result = evaluate(  \n",
    "    data=\"data.jsonl\",  \n",
    "    evaluators={\"groundedness\": groundedness_eval},  \n",
    "    output_path=\"./eval_results.json\"    # Output is optional  \n",
    ")  \n",
    "import json  \n",
    "print(json.dumps(result['metrics'], indent=2))  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "846ec2b0",
   "metadata": {},
   "source": [
    "Data Requirements\n",
    " \n",
    "\n",
    "Each line in .jsonl must be a valid JSON object.\n",
    "Key names should match the evaluator's expected input (query, response, context, etc)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45b60261",
   "metadata": {},
   "source": [
    "7. Evaluating Conversations\n",
    " \n",
    "Conversation Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecfab128",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.evaluation import GroundednessEvaluator  \n",
    "  \n",
    "conversation = {  \n",
    "    \"messages\": [  \n",
    "        {\"content\": \"Which tent is the most waterproof?\", \"role\": \"user\"},  \n",
    "        {  \n",
    "            \"content\": \"The Alpine Explorer Tent is the most waterproof\",  \n",
    "            \"role\": \"assistant\",  \n",
    "            \"context\": \"From our product list the Alpine Explorer Tent is the most waterproof.\",  \n",
    "        },  \n",
    "        {\"content\": \"How much does it cost?\", \"role\": \"user\"},  \n",
    "        {  \n",
    "            \"content\": \"The Alpine Explorer Tent is $120.\",  \n",
    "            \"role\": \"assistant\",  \n",
    "            \"context\": None,  \n",
    "        },  \n",
    "    ]  \n",
    "}  \n",
    "  \n",
    "groundedness_eval = GroundednessEvaluator(model_config)  \n",
    "score = groundedness_eval(conversation=conversation)  \n",
    "  \n",
    "import json  \n",
    "print(json.dumps(score, indent=2))  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f7d013d",
   "metadata": {},
   "source": [
    "JSONL Format for Conversations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f45d0e9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "{\"conversation\": { \"messages\": [...] }}  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b58b3b78",
   "metadata": {},
   "source": [
    "8. Using Composite Evaluators\n",
    " \n",
    "Composite evaluators group several metrics under one evaluator:\n",
    "\n",
    "QA Evaluator Example (works on query-response pairs):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aa06ed4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_data = [\n",
    "    {\n",
    "        \"query\": \"Who invented the lightbulb?\",\n",
    "        \"response\": \"Thomas Edison invented the first commercially successful incandescent light bulb.\",\n",
    "        \"context\": \"In 1879, Thomas Edison created the first commercially successful incandescent light bulb.\"\n",
    "    },\n",
    "    # Add more entries as needed\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5eb967c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\"evaluation_data.jsonl\", \"w\") as f:\n",
    "    for entry in evaluation_data:\n",
    "        f.write(json.dumps(entry) + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c811cb0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.evaluation import evaluate, QAEvaluator\n",
    "\n",
    "# Initialize your evaluator\n",
    "qa_evaluator = QAEvaluator(model_config)\n",
    "\n",
    "# Run the evaluation\n",
    "result = evaluate(\n",
    "    data=\"evaluation_data.jsonl\",\n",
    "    evaluators={\"qa\": qa_evaluator},\n",
    "    evaluation_name=\"RAG Evaluation Demo\"\n",
    ")\n",
    "\n",
    "print(result[\"metrics\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d8ee371",
   "metadata": {},
   "outputs": [],
   "source": [
    "!az login\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5cb6088",
   "metadata": {},
   "source": [
    "QA Evaluator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aae750c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.evaluation import evaluate, QAEvaluator\n",
    "\n",
    "# Define your Azure AI project details\n",
    "azure_ai_project = {\n",
    "    \"subscription_id\": \"your sub id\",\n",
    "     \"project_name\": \"your project name\",\n",
    "     \"resource_group_name\": \"your resource group name\"\n",
    "}\n",
    "\n",
    "# Initialize your evaluator\n",
    "qa_evaluator = QAEvaluator(model_config)\n",
    "\n",
    "# Run the evaluation\n",
    "result = evaluate(\n",
    "    data=\"evaluation_data.jsonl\",\n",
    "    evaluators={\"qa\": qa_evaluator},\n",
    "    evaluation_name=\"RAG Evaluation Demo\",\n",
    "    azure_ai_project=azure_ai_project\n",
    ")\n",
    "\n",
    "# Output the evaluation metrics and the link to Azure AI Foundry\n",
    "print(result[\"metrics\"])\n",
    "print(f\"View results in Azure AI Foundry: {result.get('studio_url')}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55ba6d19",
   "metadata": {},
   "source": [
    "Groundedness Evaluator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c4108e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.evaluation import evaluate, GroundednessEvaluator\n",
    "\n",
    "# Define your Azure AI project details\n",
    "azure_ai_project = {\n",
    "    \"subscription_id\": \"your sub id\",\n",
    "     \"project_name\": \"your project name\",\n",
    "     \"resource_group_name\": \"your resource group name\"\n",
    "}\n",
    "\n",
    "# Initialize the evaluator\n",
    "groundedness_evaluator = GroundednessEvaluator(model_config)\n",
    "\n",
    "# Run the evaluation\n",
    "result = evaluate(\n",
    "    data=\"evaluation_data.jsonl\",\n",
    "    evaluators={\"groundedness\": groundedness_evaluator},\n",
    "    evaluation_name=\"RAG Groundedness Evaluation\",\n",
    "    azure_ai_project=azure_ai_project\n",
    ")\n",
    "\n",
    "# Output the evaluation metrics and the link to Azure AI Foundry\n",
    "print(result[\"metrics\"])\n",
    "print(f\"View results in Azure AI Foundry: {result.get('studio_url')}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "845df897",
   "metadata": {},
   "source": [
    "9. Tracking Evaluations in Azure AI Project\n",
    " \n",
    "You can log evaluation runs to your Azure AI project for easier tracking:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e349d842",
   "metadata": {},
   "outputs": [],
   "source": [
    "#example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6cff137",
   "metadata": {},
   "source": [
    "Another Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3072ce7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install azure-ai-evaluation azure-identity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebdff02a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.evaluation import evaluate, GroundednessEvaluator, RetrievalEvaluator, ViolenceEvaluator, BleuScoreEvaluator\n",
    "from azure.identity import DefaultAzureCredential\n",
    "\n",
    "credential = DefaultAzureCredential()\n",
    "# Define your Azure AI project details\n",
    "azure_ai_project = {\n",
    "    \"subscription_id\": \"your sub id\",\n",
    "     \"project_name\": \"your project name\",\n",
    "     \"resource_group_name\": \"your resource group name\"\n",
    "    \n",
    "}\n",
    "\n",
    "# Initialize evaluators\n",
    "evaluators = {\n",
    "    \"groundedness\": GroundednessEvaluator(model_config),\n",
    "    \"retrieval\": RetrievalEvaluator(model_config),\n",
    "    \"violence\": ViolenceEvaluator(credential=credential, azure_ai_project=azure_ai_project),\n",
    "    \"bleu\": BleuScoreEvaluator(threshold=0.5)\n",
    "}\n",
    "\n",
    "# Run the evaluation\n",
    "result = evaluate(\n",
    "    data=\"evaluation_data_new.jsonl\",\n",
    "    evaluators=evaluators,\n",
    "    evaluation_name=\"RAG Comprehensive Evaluation\",\n",
    "    azure_ai_project=azure_ai_project\n",
    ")\n",
    "\n",
    "# Output the evaluation metrics and the link to Azure AI Foundry\n",
    "print(result[\"metrics\"])\n",
    "print(f\"View results in Azure AI Foundry: {result.get('studio_url')}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bce8160",
   "metadata": {},
   "source": [
    "10. Advanced: Local Evaluation on a Target (Optional)\n",
    " \n",
    "If you want to run evaluation against a live application (e.g., an API or callable class), provide it as the target parameter. The Evaluator will send queries to the target and evaluate the returned answers.\n",
    "\n",
    "Example (assuming you have a callable askwiki class):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b1734a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from askwiki import askwiki  # Your implementation  \n",
    "  \n",
    "result = evaluate(  \n",
    "    data=\"data.jsonl\",  \n",
    "    target=askwiki,  \n",
    "    evaluators={\"groundedness\": groundedness_eval},  \n",
    "    evaluator_config={  \n",
    "        \"default\": {  \n",
    "            \"column_mapping\": {  \n",
    "                \"query\": \"${data.query}\",  \n",
    "                \"context\": \"${outputs.context}\",  \n",
    "                \"response\": \"${outputs.response}\"  \n",
    "            }  \n",
    "        }  \n",
    "    },  \n",
    "    output_path=\"target_eval.json\"  \n",
    ")  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
