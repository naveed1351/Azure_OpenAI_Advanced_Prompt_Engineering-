{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Chat with prompty"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 0. Install dependent packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%capture --no-stderr\n",
        "%pip install promptflow-devkit"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "%pip install promptflow promptflow-tools"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Prompty:\n",
        "\n",
        "Prompty is a file with .prompty extension for developing prompt template. \n",
        "The prompty asset is a markdown file with a modified front matter. \n",
        "The front matter is in yaml format that contains a number of metadata fields which defines model configuration and expected inputs of the prompty."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1724227192462
        }
      },
      "outputs": [],
      "source": [
        "with open(\"prompty/chat.prompty\") as fin:\n",
        "    print(fin.read())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Create necessary connections\n",
        "Connection helps securely store and manage secret keys or other sensitive credentials required for interacting with LLM and other external tools for example Azure Content Safety.\n",
        "\n",
        "We need to set up the connection if we haven't added it before. After created, it's stored in local db and can be used in any flow.\n",
        "\n",
        "Prepare your Azure Open AI resource follow this [instruction](https://learn.microsoft.com/en-us/azure/cognitive-services/openai/how-to/create-resource?pivots=web-portal) and get your `api_key` if you don't have one."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "#%pip install keyrings.alt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from promptflow.client import PFClient\n",
        "from promptflow.connections import AzureOpenAIConnection\n",
        "\n",
        "client = PFClient()\n",
        "\n",
        "connection = AzureOpenAIConnection(\n",
        "    name=\"my_azure_open_ai_connection\",\n",
        "    api_key=\"YOUR_API_KEY\",\n",
        "    api_base=\"https://YOUR_RESOURCE_NAME.openai.azure.com\"\n",
        ")\n",
        "\n",
        "result = client.connections.create_or_update(connection)\n",
        "print(\"Connection created or updated:\", result)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1724227207417
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "conn_name = \"my_azure_open_ai_connection\"\n",
        "conn = client.connections.get(name=conn_name)\n",
        "print(\"using this connection :\",conn_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Execute prompty as function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1724227211381
        }
      },
      "outputs": [],
      "source": [
        "from promptflow.core import Prompty\n",
        "\n",
        "# load prompty as a flow\n",
        "f = Prompty.load(\"prompty/chat.prompty\")\n",
        "# execute the flow as function\n",
        "question = \"What is the capital of France?\"\n",
        "result = f(question=question)\n",
        "result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "You can override connection with `AzureOpenAIModelConfiguration` and `OpenAIModelConfiguration`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1724227224344
        }
      },
      "outputs": [],
      "source": [
        "from promptflow.core import AzureOpenAIModelConfiguration, OpenAIModelConfiguration\n",
        "\n",
        "\n",
        "# override configuration with created connection in AzureOpenAIModelConfiguration\n",
        "configuration = AzureOpenAIModelConfiguration(\n",
        "    connection=\"my_azure_open_ai_connection\", azure_deployment=\"gpt-4o\"\n",
        ")\n",
        "\n",
        "# override openai connection with OpenAIModelConfiguration\n",
        "# configuration = OpenAIModelConfiguration(\n",
        "#     connection=connection,\n",
        "#     model=\"gpt-3.5-turbo\"\n",
        "# )\n",
        "\n",
        "override_model = {\n",
        "    \"configuration\": configuration,\n",
        "}\n",
        "\n",
        "# load prompty as a flow\n",
        "f = Prompty.load(\"prompty/chat.prompty\", model=override_model)\n",
        "# execute the flow as function\n",
        "question = \"What is the capital of France?\"\n",
        "result = f(question=question)\n",
        "result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Visualize trace by using start_trace"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1724226907338
        }
      },
      "outputs": [],
      "source": [
        "from promptflow.tracing import start_trace\n",
        "\n",
        "# start a trace session, and print a url for user to check trace\n",
        "start_trace()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Re-run below cell will collect a trace in trace UI."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1724226911351
        }
      },
      "outputs": [],
      "source": [
        "# rerun the function, which will be recorded in the trace\n",
        "result = f(question=question)\n",
        "result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Eval the result \n",
        "\n",
        "In this example, we will use a prompt that determines whether a chat conversation contains an apology from the assistant."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1724227565334
        }
      },
      "outputs": [],
      "source": [
        "eval_prompty = \"prompty/apology.prompty\"\n",
        "\n",
        "with open(eval_prompty) as fin:\n",
        "    print(fin.read())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Note: the eval flow returns a `json_object`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1724227675353
        }
      },
      "outputs": [],
      "source": [
        "# load prompty as a flow\n",
        "eval_flow = Prompty.load(eval_prompty)\n",
        "# execute the flow as function\n",
        "result = eval_flow(question=question, answer=result, messages=[])\n",
        "result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "!pf service stop"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Batch run with multi-line data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1724228576093
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "from promptflow.client import PFClient\n",
        "\n",
        "flow = \"./prompty/chat.prompty\"  # path to the prompty file\n",
        "data = \"./prompty/data.jsonl\"  # path to the data file\n",
        "\n",
        "# create run with the flow and data\n",
        "pf = PFClient()\n",
        "base_run = pf.run(\n",
        "    flow=flow,\n",
        "    data=data,\n",
        "    column_mapping={\n",
        "        \"question\": \"${data.question}\",\n",
        "        \"chat_history\": \"${data.chat_history}\",\n",
        "    },\n",
        "    stream=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1724228583471
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "details = pf.get_details(base_run)\n",
        "details"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1724228589908
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "details.head(3)['outputs.output'].values"
      ]
    }
  ],
  "metadata": {
    "build_doc": {
      "author": [
        "lalala123123@github.com",
        "wangchao1230@github.com"
      ],
      "category": "local",
      "section": "Prompty",
      "weight": 20
    },
    "description": "A quickstart tutorial to run a chat prompty and evaluate it.",
    "kernel_info": {
      "name": "python310-sdkv2"
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    },
    "microsoft": {
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      },
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      }
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    },
    "resources": "examples/requirements.txt, examples/prompty/chat-basic, examples/prompty/eval-apology"
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
