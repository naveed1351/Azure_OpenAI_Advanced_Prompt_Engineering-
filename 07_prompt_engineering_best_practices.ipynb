{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Note: Below is only for managed Identity:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "gather": {
          "logged": 1720097684406
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# import openai\n",
        "# from openai import AzureOpenAI\n",
        "# import os \n",
        "# from azure.identity import ManagedIdentityCredential\n",
        "\n",
        "# default_credential=ManagedIdentityCredential(client_id=\"XXX\")\n",
        "# token=default_credential.get_token(\"https://cognitiveservices.azure.com/.default\")\n",
        "# Resource_endpoint=\"XXX\"\n",
        "\n",
        "# client = AzureOpenAI(\n",
        "#   azure_endpoint = Resource_endpoint, \n",
        "#   api_key=token.token,  \n",
        "#   api_version=\"2023-05-15\"\n",
        "# )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Install libraries:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 103,
      "metadata": {},
      "outputs": [],
      "source": [
        "#pip install python-dotenv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "#pip install openai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "gather": {
          "logged": 1724377457848
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import openai\n",
        "from openai import AzureOpenAI\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "# Set up Azure OpenAI\n",
        "load_dotenv(\"credentials.env\")\n",
        "\n",
        "openai.api_type = \"azure\"\n",
        "    \n",
        "client = AzureOpenAI(\n",
        "    api_key=os.getenv(\"AZURE_OPENAI_KEY\"),  \n",
        "    api_version=\"2025-01-01-preview\",\n",
        "    azure_endpoint = os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "gather": {
          "logged": 1724376603988
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "#print(os.getenv(\"AZURE_OPENAI_ENDPOINT\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## 1. Understand the AOAI Models' capabilities. Start with the latest model, prove your idea , then test with smaller models. \n",
        "Model size is critical for better performance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## 2. Be specific, descriptive and as detailed as possible about the desired context, outcome, length, format, style, etc"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "** Length ** control (specify desired output length e.g.: number of words)\n",
        "\n",
        "** Tone ** control (e.g.: polite, passionate, professional, technical, funny, casual, serious etc.)\n",
        "\n",
        "** Style ** control (e.g.: in the style of Shakespeare, JK Rowling, Nelson Mandela etc.)\n",
        "\n",
        "** Audience ** control (e.g.: a 5-year-old can understand etc)\n",
        "\n",
        "** Context ** control (e.g.: news, novel, textbook, report, white paper, blog etc.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "gather": {
          "logged": 1724220799436
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "base_system_message = \"You are a helpful assistant.\"\n",
        "system_message = f\"{base_system_message.strip()}\"\n",
        "\n",
        "# This is the first user message that will be sent to the model. Feel free to update this.\n",
        "user_message = \" Write a 2 paragraph inspiring poem focussing on products of Howden Group Holdings in a funny way.\"\n",
        "# Write a 2 paragraph inspiring poem about Howden Group Holdings\n",
        "# Write a 2 paragraph inspiring poem focussing on products of Howden Group Holdings in a funny way\n",
        "# Write a 2 paragraph inspiring poem focussing on products of Howden Group Holdings in the style of Shakespeare"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1724220812307
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# Instead of appending, writing messages in the SDK\n",
        "response = client.chat.completions.create(\n",
        "    model=\"gpt-4o\", # model = \"deployment_name\".\n",
        "    messages=[\n",
        "        {\"role\": \"system\", \"content\": system_message},\n",
        "        {\"role\": \"user\", \"content\": user_message}\n",
        "    ]\n",
        ")\n",
        "\n",
        "print(response.choices[0].message.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## 3. Put instructions at the begining of the prompt and use ### or \"\"\" to separate the instruction and context"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "gather": {
          "logged": 1724220821426
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "text = f\"\"\"\n",
        "We’re happy to announce that OpenAI and Microsoft are extending our partnership.\\\n",
        "This multi-year, multi-billion dollar investment from Microsoft follows their previous investments \\\n",
        "in 2019 and 2021, and will allow us to continue our independent research and develop AI that is \\\n",
        "increasingly safe, useful, and powerful. \\n\\n \\\n",
        "In pursuit of our mission to ensure advanced AI benefits all of humanity, OpenAI remains a \\\n",
        "capped-profit company and is governed by the OpenAI non-profit. This structure allows us to \\\n",
        "raise the capital we need to fulfill our mission without sacrificing our core beliefs about \\\n",
        "broadly sharing benefits and the need to prioritize safety. \\\n",
        "Microsoft shares this vision and our values, and our partnership is instrumental to our progress.\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "gather": {
          "logged": 1724220822459
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "prompt = f\"\"\"\n",
        "Summarize the text delimited by hashtags as a bullet point list of the most important points.\n",
        "###{text}###\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1724220824323
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "response = client.chat.completions.create(\n",
        "    model=\"gpt-4o\",  # Ensure this is the correct model identifier\n",
        "    messages=[\n",
        "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "        {\"role\": \"user\", \"content\": prompt}\n",
        "    ],\n",
        "    temperature=0,\n",
        "    max_tokens=60\n",
        ")\n",
        "\n",
        "# Print the result\n",
        "print(response.choices[0].message.content.strip())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## 4. Articulate the desired output format through examples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "gather": {
          "logged": 1724220830430
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "system_message = \"You are a helpful assistant.\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "gather": {
          "logged": 1724220834461
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "user_message=f\"\"\"Extract the entities mentioned in the text below. \n",
        "First extract all company names, then extract all years, \n",
        "then extract specific topics which fit the content and finally extract general overarching themes\\n\\n \n",
        "Desired format: \n",
        "Company names: <comma_separated_list_of_company_names> \n",
        "Years: \n",
        "Specific topics:\n",
        "General themes: \n",
        "### Text:\n",
        "We’re happy to announce that OpenAI and Microsoft are extending our partnership.\n",
        "This multi-year, multi-billion dollar investment from Microsoft follows their previous investments \n",
        "in 2019 and 2021, and will allow us to continue our independent research and develop AI that is \n",
        "increasingly safe, useful, and powerful. \\n\\n \n",
        "###\n",
        "\"\"\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1724220843321
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# Instead of appending, writing messages in the SDK\n",
        "response = client.chat.completions.create(\n",
        "    model=\"gpt-4o\", # model = \"deployment_name\".\n",
        "    messages=[\n",
        "        {\"role\": \"system\", \"content\": system_message},\n",
        "        {\"role\": \"user\", \"content\": user_message}\n",
        "    ]\n",
        ")\n",
        "\n",
        "print(response.choices[0].message.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## 5.Start with zero-shot, then few-shot (example)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 117,
      "metadata": {
        "gather": {
          "logged": 1724220850411
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "system_message = \"You are a helpful assistant.\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 118,
      "metadata": {
        "gather": {
          "logged": 1724220851415
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "prompt_zero=f\"\"\"Extract most important keywords from the corresponding texts below.\\n\\n \n",
        "\n",
        "###Text: \n",
        "We’re happy to announce that OpenAI and Microsoft are extending our partnership.\n",
        "This multi-year, multi-billion dollar investment from Microsoft follows their previous investments \n",
        "in 2019 and 2021, and will allow us to continue our independent research and develop AI that is \n",
        "increasingly safe, useful, and powerful. \\n\n",
        "Keywords:###\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1724220854326
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "response = client.chat.completions.create(\n",
        "    model=\"gpt-4o\", # model = \"deployment_name\".\n",
        "    messages=[\n",
        "        {\"role\": \"system\", \"content\": system_message},\n",
        "        {\"role\": \"user\", \"content\": prompt_zero}\n",
        "    ]\n",
        ")\n",
        "\n",
        "print(response.choices[0].message.content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 120,
      "metadata": {
        "gather": {
          "logged": 1724220856413
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "prompt_few=f\"\"\"Extract most important keywords from the corresponding texts below.\\n\\n \n",
        "### Text 1: \n",
        "Stripe provides APIs that web developers can use to integrate \n",
        "payment processing into their websites and mobile applications. \\n\n",
        "Keywords 1: Stripe, payment processing, APIs, web developers, websites \n",
        "### \n",
        "\n",
        "###Text 2: \n",
        "OpenAI has trained cutting-edge language models that are very good at understanding \n",
        "and generating text. Our API provides access to these models and can be used to solve virtually \n",
        "any task that involves processing language. \\n\n",
        "Keywords 2: OpenAI, language models, text processing, API.\n",
        "### \n",
        "\n",
        "###Text 3: \n",
        "We’re happy to announce that OpenAI and Microsoft are extending our partnership.\n",
        "This multi-year, multi-billion dollar investment from Microsoft follows their previous investments \n",
        "in 2019 and 2021, and will allow us to continue our independent research and develop AI that is \n",
        "increasingly safe, useful, and powerful. \\n\n",
        "Keywords 3:\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1724220858308
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# Instead of appending, writing messages in the SDK\n",
        "response = client.chat.completions.create(\n",
        "    model=\"gpt-4o\", # model = \"deployment_name\".\n",
        "    messages=[\n",
        "        {\"role\": \"system\", \"content\": system_message},\n",
        "        {\"role\": \"user\", \"content\": prompt_few}\n",
        "    ]\n",
        ")\n",
        "\n",
        "print(response.choices[0].message.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## 6.Instead of just saying what not to do, say what to do instead"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "gather": {
          "logged": 1724220863425
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "system_message= f\"\"\"You are an agent trying to diagnose the problem and suggest a solution, whilst refraining from asking any questions related to PII. \n",
        "Instead of asking for PII, such as username or password, refer the user to the help article www.samplewebsite.com/help/faq \\n\\n\"\"\"\n",
        "\n",
        "# This is the first user message that will be sent to the model. Feel free to update this.\n",
        "user_message = \"I can’t log in to my account.\"\n",
        "\n",
        "# Create the list of messages. role can be either \"user\" or \"assistant\" \n",
        "messages=[\n",
        "    {\"role\": \"system\", \"content\": system_message},\n",
        "    {\"role\": \"user\", \"name\":\"example_user\", \"content\": user_message}\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1724220873329
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "response = client.chat.completions.create(\n",
        "    model=\"gpt-4o\", # model = \"deployment_name\".\n",
        "    messages=[\n",
        "        {\"role\": \"system\", \"content\": system_message},\n",
        "        {\"role\": \"user\", \"content\": user_message}\n",
        "    ]\n",
        ")\n",
        "\n",
        "print(response.choices[0].message.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## 7. Divide complex tasks into sub-tasks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "gather": {
          "logged": 1720098687167
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "system_message = \"You are a helpful assistant.\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "gather": {
          "logged": 1724221069428
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "text = f\"\"\"\n",
        "As an FSI company, we want our pension schemes to have a positive impact on our customers. Whether you're just starting to save into a pension or ready to take money out of it, we have the best interests of our members and customers at heart. It's about long-term financial wellbeing and sharing responsibility for building a better future. Save today to enjoy tomorrow.\"\"\"\n",
        "# example 1\n",
        "user_message = f\"\"\"\n",
        "Perform the actions below by separating your answers with line breaks. \n",
        "1 - Summarize the following text below with 1 sentence in English.\n",
        "2 - Translate the summary into Hindi.\n",
        "3 - List each company name in the Hindi summary.\n",
        "4 - Output a json object that contains the following:\n",
        "keys: Hindi_summary, Hindi_company_names.\n",
        "\n",
        "\n",
        "###\n",
        "Text:\n",
        "{text} \n",
        "###\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1724221072306
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "response = client.chat.completions.create(\n",
        "    model=\"gpt-4o\", # model = \"deployment_name\".\n",
        "    messages=[\n",
        "        {\"role\": \"system\", \"content\": system_message},\n",
        "        {\"role\": \"user\", \"content\": user_message}\n",
        "    ]\n",
        ")\n",
        "\n",
        "print(response.choices[0].message.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## 8. Chain of Thought"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "The language model is prompted to generate a few intermediate reasoning steps to arrive at the final answer. \n",
        "\n",
        "Uses \"greedy decoding\" which means selecting the most likely token (word or character) at each step of the sequence generation process. At each time step, the model predicts the next token based on the previously generated tokens, and the token with the highest predicted probability is chosen as the output for that step. This process is repeated until the desired sequence length is reached or until a special end-of-sequence token is generated.\n",
        "\n",
        "**Temp=0** is used because it uses greedy decoding. It first creates the greedy coding and then the answer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "gather": {
          "logged": 1724221351442
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# This prompt gets wrong answer\n",
        "\n",
        "PROMPT_ZERO_SHOT = \"\"\"Q: Roger has 5 tennis balls. He buys 2 more cans of tennis\n",
        "balls. Each can has 3 tennis balls. He gives 4 of them to his friend. How many tennis balls does\n",
        "he have now?\n",
        "A: The answer (arabic numerals) is\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1724221352453
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "response = client.chat.completions.create(\n",
        "    model=\"gpt-4o\",  # Ensure this is the correct model identifier\n",
        "    messages=[\n",
        "        {\"role\": \"system\", \"content\": system_message},\n",
        "        {\"role\": \"user\", \"content\": PROMPT_ZERO_SHOT}\n",
        "    ],\n",
        "    temperature=0,\n",
        "    max_tokens=200,\n",
        "    stop=[\"\\nQ:\"]\n",
        ")\n",
        "\n",
        "# Print the result\n",
        "print(response.choices[0].message.content.strip())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "gather": {
          "logged": 1724221354423
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "PROMPT_ZERO_SHOT_CoT = \"\"\"Q:Roger has 5 tennis balls. He buys 2 more cans of tennis\n",
        "balls. Each can has 3 tennis balls. He gives 4 of them to his friend. How many tennis balls does\n",
        "he have now?\n",
        "A: Let’s think step by step.\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1724221356305
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "response = client.chat.completions.create(\n",
        "    model=\"gpt-4o\",  # Ensure this is the correct model identifier\n",
        "    messages=[\n",
        "        {\"role\": \"system\", \"content\": system_message},\n",
        "        {\"role\": \"user\", \"content\": PROMPT_ZERO_SHOT_CoT}\n",
        "    ],\n",
        "    temperature=0,\n",
        "    max_tokens=300,\n",
        "    stop=[\"\\nQ:\"]\n",
        ")\n",
        "\n",
        "# Print the result\n",
        "print(response.choices[0].message.content.strip())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "gather": {
          "logged": 1724221989399
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "prompt_fsi_zeroshot_cot= \"\"\"Q: Let’s analyze the financial health of Company X. The company has a revenue of $10 million, expenses of $7 million, and a debt of $2 million. Calculate the net profit and the debt-to-equity ratio.\n",
        "A: The answer is\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1724221991308
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "response = client.chat.completions.create(\n",
        "    model=\"gpt-4o\",  # Ensure this is the correct model identifier\n",
        "    messages=[\n",
        "        {\"role\": \"system\", \"content\": system_message},\n",
        "        {\"role\": \"user\", \"content\": prompt_fsi_zeroshot_cot}\n",
        "    ],\n",
        "    temperature=0,\n",
        "    max_tokens=100,\n",
        "    stop=[\"\\nQ:\"]\n",
        ")\n",
        "\n",
        "# Print the result\n",
        "print(response.choices[0].message.content.strip())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "gather": {
          "logged": 1724376683955
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "prompt_fsi_zeroshot_cot= \"\"\"Q: Let’s analyze the financial health of Company X. The company has a revenue of $10 million, expenses of $7 million, and a debt of $2 million. Calculate the net profit and the debt-to-equity ratio.\n",
        "A: Let's think step by step.\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1724376688127
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "response = client.chat.completions.create(\n",
        "    model=\"gpt-4o\",  # Ensure this is the correct model identifier\n",
        "    messages=[\n",
        "        {\"role\": \"system\", \"content\": system_message},\n",
        "        {\"role\": \"user\", \"content\": prompt_fsi_zeroshot_cot}\n",
        "    ],\n",
        "    temperature=0,\n",
        "    max_tokens=1000,\n",
        "    stop=[\"\\nQ:\"]\n",
        ")\n",
        "\n",
        "# Print the result\n",
        "print(response.choices[0].message.content.strip())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "gather": {
          "logged": 1724222038404
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "PROMPT_FEW_SHOT_CoT = \"\"\"\n",
        "Q: Elif went to market with £10 and consumed £2. How much does she have now?\n",
        "A: Elif had £10 at the beginning. When she consumed £2, 10-2=8 , £8 remains.\n",
        "Q:Roger has 5 tennis balls. He buys 2 more cans of tennis\n",
        "balls. Each can has 3 tennis balls. He gives 4 of them to his friend. How many tennis balls does\n",
        "he have now?\n",
        "A:\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1724222040301
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "response = client.chat.completions.create(\n",
        "    model=\"gpt-4o\",  # Ensure this is the correct model identifier\n",
        "    messages=[\n",
        "        {\"role\": \"system\", \"content\": system_message},\n",
        "        {\"role\": \"user\", \"content\": PROMPT_FEW_SHOT_CoT}\n",
        "    ],\n",
        "    temperature=0,\n",
        "    max_tokens=1000,\n",
        "    stop=[\"\\nQ:\"]\n",
        ")\n",
        "\n",
        "# Print the result\n",
        "print(response.choices[0].message.content.strip())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "** Auto-COT ** uses zero-shot-cot results just like few-shot learning for reasoning. Instead of using few-shot-cot, auto-cot can be useful and easy because you don't need to create manual examples (labels/reasonings)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {
        "gather": {
          "logged": 1724222044453
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "prompt_auto_cot = \"\"\"\n",
        "Q:Roger has 5 tennis balls. He buys 2 more cans of tennis\n",
        "balls. Each can has 3 tennis balls. He gives 4 of them to his friend. How many tennis balls does\n",
        "he have now?\n",
        "A: Lets think step by step.Roger had 5 tennis balls at the beginning. He bought 2 cans of tennis balls, each with 3 balls, so he now has 5+2x3=11 tennis balls. After giving 4 to his friend, he has 11-4=7 tennis balls remaining.\n",
        "Q: Elif went to market with £10 and consumed £2. How much does she have now?\n",
        "A:\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1724222046311
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "response = client.chat.completions.create(\n",
        "    model=\"gpt-4o\",  # Ensure this is the correct model identifier\n",
        "    messages=[\n",
        "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "        {\"role\": \"user\", \"content\": prompt_auto_cot}\n",
        "    ],\n",
        "    temperature=0.5,\n",
        "    max_tokens=1000\n",
        ")\n",
        "\n",
        "# Access the message content safely\n",
        "message_content = response.choices[0].message.content\n",
        "if message_content:\n",
        "    print(message_content.strip())\n",
        "else:\n",
        "    print(\"The response content is empty.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## 9. Self Consistency\n",
        "\n",
        "Self-consistency aims \"to replace the naive greedy decoding used in chain-of-thought prompting\". The idea is to sample multiple, diverse reasoning paths through few-shot CoT, and use the generations to ** select the most consistent answer.**\n",
        "\n",
        "In the chat scenarios, **Asking the model to self-verify** its own responses. Like a student double-checking their answers, the AI model cross-references its responses to maintain consistency. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "gather": {
          "logged": 1724222059438
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "prompt= f\"\"\"When I was 6, my sister was half my age. Now\n",
        "I am 70 how old is my sister?\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "response = client.chat.completions.create(\n",
        "    model=\"gpt-4o\",  # Ensure this is the correct model identifier\n",
        "    messages=[\n",
        "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "        {\"role\": \"user\", \"content\": prompt}\n",
        "    ],\n",
        "    temperature=0.5,\n",
        "    max_tokens=230,\n",
        "    stop=[\"\\nA:\"]\n",
        ")\n",
        "\n",
        "# Print the result\n",
        "print(response.choices[0].message.content.strip())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "gather": {
          "logged": 1724222088434
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "prompt2= f\"\"\"When I was 6, my sister was half my age. Now\n",
        "I am 70 how old is my sister? Let's think step by step\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "response = client.chat.completions.create(\n",
        "    model=\"gpt-4o\",  # Ensure this is the correct model identifier\n",
        "    messages=[\n",
        "        {\"role\": \"system\", \"content\": system_message},\n",
        "        {\"role\": \"user\", \"content\": prompt2}\n",
        "    ],\n",
        "    temperature=0,\n",
        "    max_tokens=1000\n",
        ")\n",
        "\n",
        "# Print the result\n",
        "print(response.choices[0].message.content.strip())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "gather": {
          "logged": 1724222112505
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "prompt3=f\"\"\"\n",
        "Q: There are 15 trees in the grove. Grove workers will plant trees in the grove today. After they are done,\n",
        "there will be 21 trees. How many trees did the grove workers plant today?\n",
        "A: We start with 15 trees. Later we have 21 trees. The difference must be the number of trees they planted.\n",
        "So, they must have planted 21 - 15 = 6 trees. The answer is 6.\n",
        "Q: If there are 3 cars in the parking lot and 2 more cars arrive, how many cars are in the parking lot?\n",
        "A: There are 3 cars in the parking lot already. 2 more arrive. Now there are 3 + 2 = 5 cars. The answer is 5.\n",
        "Q: Leah had 32 chocolates and her sister had 42. If they ate 35, how many pieces do they have left in total?\n",
        "A: Leah had 32 chocolates and Leah’s sister had 42. That means there were originally 32 + 42 = 74\n",
        "chocolates. 35 have been eaten. So in total they still have 74 - 35 = 39 chocolates. The answer is 39.\n",
        "Q: Jason had 20 lollipops. He gave Denny some lollipops. Now Jason has 12 lollipops. How many lollipops\n",
        "did Jason give to Denny?\n",
        "A: Jason had 20 lollipops. Since he only has 12 now, he must have given the rest to Denny. The number of\n",
        "lollipops he has given to Denny must have been 20 - 12 = 8 lollipops. The answer is 8.\n",
        "Q: Shawn has five toys. For Christmas, he got two toys each from his mom and dad. How many toys does\n",
        "he have now?\n",
        "A: He has 5 toys. He got 2 from mom, so after that he has 5 + 2 = 7 toys. Then he got 2 more from dad, so\n",
        "in total he has 7 + 2 = 9 toys. The answer is 9.\n",
        "Q: There were nine computers in the server room. Five more computers were installed each day, from\n",
        "monday to thursday. How many computers are now in the server room?\n",
        "A: There are 4 days from monday to thursday. 5 computers were added each day. That means in total 4 * 5 =\n",
        "20 computers were added. There were 9 computers in the beginning, so now there are 9 + 20 = 29 computers.\n",
        "The answer is 29.\n",
        "Q: Michael had 58 golf balls. On tuesday, he lost 23 golf balls. On wednesday, he lost 2 more. How many\n",
        "golf balls did he have at the end of wednesday?\n",
        "A: Michael initially had 58 balls. He lost 23 on Tuesday, so after that he has 58 - 23 = 35 balls. On\n",
        "Wednesday he lost 2 more so now he has 35 - 2 = 33 balls. The answer is 33.\n",
        "Q: Olivia has $23. She bought five bagels for $3 each. How much money does she have left?\n",
        "A: She bought 5 bagels for $3 each. This means she spent 5\n",
        "Q: When I was 6 my sister was half my age. Now I’m 70 how old is my sister?\n",
        "A:\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "gather": {
          "logged": 1724222502411
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "from openai import OpenAI\n",
        "\n",
        "def call_openai(num_times, start_phrase, temperature):\n",
        "    # Initialize the OpenAI client\n",
        "    #client = OpenAI(api_key='your-api-key')  # Replace with your actual API key\n",
        "\n",
        "    for i in range(num_times):\n",
        "        # Define the messages for the chat completion\n",
        "        messages = [\n",
        "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "            {\"role\": \"user\", \"content\": start_phrase}\n",
        "        ]\n",
        "\n",
        "        # Send a chat completion request\n",
        "        response = client.chat.completions.create(\n",
        "            model=\"gpt-4o\",  # Use the appropriate model name\n",
        "            messages=messages,\n",
        "            temperature=temperature,\n",
        "            max_tokens=100\n",
        "        )\n",
        "\n",
        "        # Extract and print the assistant's reply\n",
        "        reply = response.choices[0].message.content.strip()\n",
        "        print(f\"Response {i + 1}:\\n{reply}\")\n",
        "        print(\"*****************************\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1724222512293
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "call_openai(10, prompt3, temperature = 1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# 9. Step-Back Prompting Technique"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "You start by providing the model with a prompt or question.\n",
        "The model generates a response based on the initial prompt.\n",
        "Instead of immediately accepting the response, you prompt the model to review or analyse its own response. This could involve asking the model to check for errors, verify facts, or consider alternative approaches.\n",
        "Based on the reassessment, the model generates a refined or corrected response."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "gather": {
          "logged": 1724377472419
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# function to generate a response based on a prompt  \n",
        "def generate_response(prompt):  \n",
        "    messages = [  \n",
        "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},  \n",
        "        {\"role\": \"user\", \"content\": prompt}  \n",
        "    ]  \n",
        "    response = client.chat.completions.create(  \n",
        "        model=\"gpt-4o\",  \n",
        "        messages=messages  \n",
        "    )  \n",
        "    return response.choices[0].message.content.strip()  \n",
        "\n",
        "# function to rephrase the input text  \n",
        "def rephrase_input(input_text):  \n",
        "    rephrase_prompt = (  \n",
        "        \"You are a helpful assistant. Please rephrase the following request to make it clearer and a more general question: \"  \n",
        "        f\"'{input_text}'\"  \n",
        "    )  \n",
        "    messages = [  \n",
        "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},  \n",
        "        {\"role\": \"user\", \"content\": rephrase_prompt}  \n",
        "    ]  \n",
        "    response = client.chat.completions.create(  \n",
        "        model=\"gpt-4o\",  \n",
        "        messages=messages  \n",
        "    )  \n",
        "    return response.choices[0].message.content.strip()  \n",
        "  \n",
        "  \n",
        "# function to reword and check the answer  \n",
        "def reword_and_check_answer(original_answer):  \n",
        "    rewording_prompt = f\"Reword and verify the following statement: '{original_answer}'\"  \n",
        "    messages = [  \n",
        "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},  \n",
        "        {\"role\": \"user\", \"content\": rewording_prompt}  \n",
        "    ]  \n",
        "    response = client.chat.completions.create(  \n",
        "        model=\"gpt-4o\",  \n",
        "        messages=messages  \n",
        "    )  \n",
        "    return response.choices[0].message.content.strip()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1724377593955
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "initial_prompt = \"Howden Group Holdings, a global insurance group with employee at their heart and a focus on innovation, is committed to providing the best insurance solutions for its clients.\"  \n",
        "  \n",
        "# Generate a response based on the initial prompt  \n",
        "response = generate_response(initial_prompt)  \n",
        "print(\"*** Initial Response:\\n\", response)  \n",
        "  \n",
        "# Reword and verify the answer  \n",
        "checked_response = reword_and_check_answer(response)  \n",
        "print(\"*** Reworded and Checked Answer:\\n\", checked_response)  \n",
        "  \n",
        "print(\"*** Final Response:\\n\", checked_response)  \n",
        "  \n",
        "# if __name__ == \"__main__\":  \n",
        "#     initial_prompt = input(\"Enter your question: \")  \n",
        "#     response = generate_response(initial_prompt)  \n",
        "#     print(\"Initial Response:\\n\", response)  \n",
        "#     checked_response = reword_and_check_answer(response)  \n",
        "#     print(\"Reworded and Checked Answer:\\n\", checked_response)  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## 10. Iterative approach\n",
        "\n",
        "Prompt engineering is an iterative process. If you're unsatisfied with the AI's response, refine your prompt and try again. Analyze the results you receive and consider adjusting your prompt's context, clarity, or structure. This process of trial and error will help you better understand how the AI model interprets your prompts and allow you to fine-tune your approach.\n",
        "\n",
        "·        Try different prompts to find what works best\n",
        "\n",
        "·        When attempting few-shot learning, try also to include direct instructions\n",
        "\n",
        "·        Rephrase a direct instruction set to be more or less concise, e.g.: taking a previous example and giving the next instruction without having to repeat the input\n",
        "\n",
        "·        Try different personas keywords to see how it affects the response style\n",
        "\n",
        "·        Use fewer or more examples in the few-shot learning\n",
        "\n",
        "·        Co-create with AI: An example of a very useful prompt to get a good output from the LLM :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 95,
      "metadata": {
        "gather": {
          "logged": 1724222541409
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "text=f\"\"\"\n",
        "Overall Results\n",
        "Year Ended December 31, 2023 versus 2022:\n",
        "Net income attributable to  ordinary shareholders was $1.1 billion for the year ended December 31, 2023, \n",
        "which compares to a net loss of $906 million from 2022, as a result of:\n",
        "• Favorable total investment returns recognized in net income of $1.1 billion for the year ended December 31, \n",
        "2023, consisting of the aggregate of net investment income, net realized (losses) gains, net unrealized gains \n",
        "(losses) and income (losses) from equity method investments, in comparison to negative total investment \n",
        "returns included in net income of $1.2 billion for the year ended December 31, 2022. The variance in total \n",
        "investment returns recognized in net income was driven by: \n",
        "◦ Net unrealized gains on our other investments, including equities of $397 million, in comparison to net \n",
        "unrealized losses in 2022 of $433 million, as a result of strong global equity market performance, \n",
        "particularly in the first and fourth quarters of 2023, and tightening high yield credit spreads, in comparison to \n",
        "the challenging market environment for the year ended December 31, 2022;\n",
        "◦ Net realized and unrealized gains on our fixed maturities of $66 million in 2023, compared to net realized \n",
        "and unrealized losses of $1.2 billion in 2022, primarily due to a decrease in interest rates across U.S., U.K. \n",
        "and European markets in 2023 as compared to significant increases in interest rates in 2022; \n",
        "◦ An increase in net investment income of $192 million in 2023 when compared to 2022, consistent with the \n",
        "increasing investment income we have earned on a sequential quarterly basis, primarily due to the \n",
        "reinvestment of fixed maturities at higher yields, deployment of consideration received from LPT and \n",
        "insurance transactions closed over the past 12 months and the impact of rising interest rates on our fixed \n",
        "maturities securities that are subject to floating interest rates; and\n",
        "◦ Income from equity method investments of $13 million, driven by income from our investments in Core \n",
        "Specialty and Citco, partially offset by losses from our investment in Monument Re, compared to losses of \n",
        "$74 million in 2022, primarily driven by losses from our investment in Monument Re. \n",
        "• An increase in other income of $241 million in 2023 when compared to 2022, largely driven by the first quarter \n",
        "2023 net gain recognized from the novation of the Enhanzed Re reinsurance of a closed block of life annuity \n",
        "policies; and \n",
        "• A favorable change in income tax benefit of $238 million, primarily driven by the establishment of a $205 million \n",
        "net deferred tax asset related to the enactment of the Bermuda Corporate Income Tax in December 2023. We \n",
        "also recorded a $25 million partial release of our deferred tax asset valuation allowance as a result of increases \n",
        "in projected taxable income in the U.S. and a reduction in deferred tax assets associated with decreases in \n",
        "unrealized losses on investment securities reported in AOCI in the U.S. and U.K. jurisdictions. This was partially \n",
        "offset by an increase in the valuation allowance in our U.K. and EU jurisdictions primarily due to losses, \n",
        "whereby no corresponding tax benefits were recognized for the period. \n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 96,
      "metadata": {
        "gather": {
          "logged": 1724222543421
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "prompt_iterative= f\"\"\" Your task is to explain given information in a very simple way.\n",
        "### Context:\n",
        "{text}\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1724222556287
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "system_message = \"You are a helpful assistant.\"\n",
        "\n",
        "response = client.chat.completions.create(\n",
        "    model=\"gpt-4o\", # model = \"deployment_name\".\n",
        "    messages=[\n",
        "        {\"role\": \"system\", \"content\": system_message},\n",
        "        {\"role\": \"user\", \"content\": prompt_iterative}\n",
        "    ]\n",
        ")\n",
        "\n",
        "print(response.choices[0].message.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "**Issue 1:** I want to keep numerical values in more readable output format."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 98,
      "metadata": {
        "gather": {
          "logged": 1724222590423
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "prompt_iterative1= f\"\"\" Your task is to organize given information in table format by keeping numarical values. \n",
        "\n",
        "### Context:\n",
        "{text}\n",
        "###\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1724222601329
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "system_message = \"You are a helpful assistant.\"\n",
        "\n",
        "response = client.chat.completions.create(\n",
        "    model=\"gpt-4o\", # model = \"deployment_name\".\n",
        "    messages=[\n",
        "        {\"role\": \"system\", \"content\": system_message},\n",
        "        {\"role\": \"user\", \"content\": prompt_iterative1}\n",
        "    ]\n",
        ")\n",
        "\n",
        "print(response.choices[0].message.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "**Issue 2:** It is long so I need a brief summary."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {
        "gather": {
          "logged": 1724222620386
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "prompt_iterative2= f\"\"\" \n",
        "Your task is to organize given information briefly in table format by keeping numarical values.\n",
        "\n",
        "Then, provide simple explanation by using at most 20 words.\n",
        "\n",
        "### Context:\n",
        "{text}\n",
        "###\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1724222629339
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "system_message = \"You are a helpful assistant.\"\n",
        "\n",
        "response = client.chat.completions.create(\n",
        "    model=\"gpt-4o\", # model = \"deployment_name\".\n",
        "    messages=[\n",
        "        {\"role\": \"system\", \"content\": system_message},\n",
        "        {\"role\": \"user\", \"content\": prompt_iterative2}\n",
        "    ]\n",
        ")\n",
        "\n",
        "print(response.choices[0].message.content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 102,
      "metadata": {
        "gather": {
          "logged": 1724222660465
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "prompt_iterative3= f\"\"\" Your task is to organize given information briefly in table format by keeping numarical values.\n",
        "\n",
        "Then, provide simple explanation of the given context by using at most 20 words.\n",
        "\n",
        "Format everything as HTML that can be used in a website. \n",
        "\n",
        "### Context:\n",
        "{text}\n",
        "###\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1724222674292
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "system_message = \"You are a helpful assistant.\"\n",
        "\n",
        "response = client.chat.completions.create(\n",
        "    model=\"gpt-4o\", # model = \"deployment_name\".\n",
        "    messages=[\n",
        "        {\"role\": \"system\", \"content\": system_message},\n",
        "        {\"role\": \"user\", \"content\": prompt_iterative3}\n",
        "    ]\n",
        ")\n",
        "\n",
        "print(response.choices[0].message.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Display the HTML content of the completion response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1724222704447
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "from IPython.display import display, HTML\n",
        "display(HTML(response.choices[0].message.content))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Self-Ask Prompting  \n",
        "  \n",
        "**Self-Ask** is an advanced prompting strategy where the model decomposes a complex question into follow-up sub-questions, answers each one, and then combines the answers for a final response. This is particularly effective for multi-step reasoning tasks.  \n",
        "  \n",
        "**Example:**    \n",
        "*Question: Who lived longer, Theodor Haecker or Harry Vaughan Watkins?*  \n",
        "  \n",
        "Here, the model first checks if follow-up questions are needed, generates those sub-questions, provides intermediate answers, then concludes with the final answer.  \n",
        "  \n",
        "**Prompt:**  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Harry Vaughan Watkins lived longer than Theodor Haecker.\n"
          ]
        }
      ],
      "source": [
        "# Define the self-ask prompt  \n",
        "prompt_self_ask = \"\"\"Question: Who lived longer, Theodor Haecker or Harry Vaughan Watkins?  \n",
        "  \n",
        "Are follow up questions needed here? Yes.  \n",
        "  \n",
        "Follow up: How old was Theodor Haecker when he died?  \n",
        "Intermediate answer: Theodor Haecker was 65 years old when he died.  \n",
        "  \n",
        "Follow up: How old was Harry Vaughan Watkins when he died?  \n",
        "Intermediate answer: Harry Vaughan Watkins was 69 years old when he died.  \n",
        "  \n",
        "So the final answer is:  \n",
        "\"\"\"  \n",
        "  \n",
        "system_message = \"You are a helpful assistant.\"  \n",
        "response = client.chat.completions.create(  \n",
        "    model=\"gpt-4o\",  # Replace with your deployment/model name if different  \n",
        "    messages=[  \n",
        "        {\"role\": \"system\", \"content\": system_message},  \n",
        "        {\"role\": \"user\", \"content\": prompt_self_ask}  \n",
        "    ]  \n",
        ")  \n",
        "print(response.choices[0].message.content)  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Self-reflection Prompting  \n",
        "  \n",
        "Self-reflection is a prompting technique that guides the model to review, validate, and potentially revise its own output after answering a question. This helps ensure the answer truly meets the requirements—even if the original output was mistaken or incomplete.  \n",
        "  \n",
        "Here is a sample sequence using the self-reflection approach:  \n",
        "  \n",
        "---  \n",
        "  \n",
        "### **Prompt 1: (Initial Reasoning with Chain-of-Thought)**  \n",
        "  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 1: Initial prompt asking for a step-by-step solution  \n",
        "  \n",
        "prompt_self_reflection_1 = \"\"\"Suppose I have a cabbage, a goat and a lion, and I need to get them across a river. I have a boat that can only carry myself and a single other item. I am not allowed to leave the cabbage and lion alone together, and I am not allowed to leave the lion and goat alone together.  \n",
        "  \n",
        "How can I safely get all three across?  \n",
        "Please pay attention to the details of the question and think step by step.\"\"\"  \n",
        "  \n",
        "system_message = \"You are a helpful assistant.\"  \n",
        "response_1 = client.chat.completions.create(  \n",
        "    model=\"gpt-4o\",  \n",
        "    messages=[  \n",
        "        {\"role\": \"system\", \"content\": system_message},  \n",
        "        {\"role\": \"user\", \"content\": prompt_self_reflection_1}  \n",
        "    ]  \n",
        ")  \n",
        "answer_1 = response_1.choices[0].message.content  \n",
        "print(\"Step 1 - Initial Answer:\\n\", answer_1)  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "  \n",
        "---  \n",
        "  \n",
        "### **Prompt 2: (Prompting for Self-reflection)**  \n",
        "  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 2: Reflection on the model's prior answer  \n",
        "  \n",
        "prompt_self_reflection_2 = \"Does the solution meet the assignment? If so, why? If not, why?\"  \n",
        "  \n",
        "response_2 = client.chat.completions.create(  \n",
        "    model=\"gpt-4o\",  \n",
        "    messages=[  \n",
        "        {\"role\": \"system\", \"content\": system_message},  \n",
        "        {\"role\": \"user\", \"content\": prompt_self_reflection_1},  \n",
        "        {\"role\": \"assistant\", \"content\": answer_1},  \n",
        "        {\"role\": \"user\", \"content\": prompt_self_reflection_2}  \n",
        "    ]  \n",
        ")  \n",
        "answer_2 = response_2.choices[0].message.content  \n",
        "print(\"\\nStep 2 - Reflection and (if needed) Correction:\\n\", answer_2)  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Chain of Verification (CoVe)  \n",
        "  \n",
        "Chain of Verification (CoVe) is a prompting method that guides a model to generate verification questions based on its original answers, answer those questions using evidence or external context, and finally use verified facts to correct or refine its initial answer.  \n",
        "  \n",
        "---  \n",
        "  \n",
        "### **Step 1: Baseline Prompt**  \n",
        "  \n",
        "Ask the model to answer the question concisely:  \n",
        "  \n",
        "---  \n",
        "  \n",
        "### **Step 2: Plan Verification Prompt**  \n",
        "  \n",
        "Ask the model to create verification questions based on the baseline answer:  \n",
        "  \n",
        "---  \n",
        "  \n",
        "### **Step 3: Execute Verification Prompt**  \n",
        "  \n",
        "Provide external context (search result) & have the model answer the verification questions:  \n",
        "  \n",
        "---  \n",
        "  \n",
        "### **Step 4: Final Prompt—Corrected Answer**  \n",
        "  \n",
        "Refine or correct the original answer based on verification:  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 1: Get the baseline answer  \n",
        "baseline_question = \"What was the primary cause of Mexican-American war?\"  \n",
        "system_message = \"You are a helpful assistant. Only answer what the question asked, concisely.\"  \n",
        "response_baseline = client.chat.completions.create(  \n",
        "    model=\"gpt-4o\",  \n",
        "    messages=[  \n",
        "        {\"role\": \"system\", \"content\": system_message},  \n",
        "        {\"role\": \"user\", \"content\": baseline_question}  \n",
        "    ]  \n",
        ")  \n",
        "baseline_answer = response_baseline.choices[0].message.content  \n",
        "print(\"Baseline Answer:\\n\", baseline_answer)  \n",
        "  \n",
        "# Step 2: Generate verification questions  \n",
        "plan_verification_prompt = f\"\"\"Your task is to create verification questions based on the original question and the following baseline answer. The verification questions are for checking the factual accuracy.  \n",
        "  \n",
        "Original Question: {baseline_question}  \n",
        "Baseline Answer: {baseline_answer}  \n",
        "  \n",
        "Please list the verification questions needed to check all major facts in the baseline answer.\"\"\"  \n",
        "  \n",
        "response_verification = client.chat.completions.create(  \n",
        "    model=\"gpt-4o\",  \n",
        "    messages=[  \n",
        "        {\"role\": \"system\", \"content\": system_message},  \n",
        "        {\"role\": \"user\", \"content\": plan_verification_prompt}  \n",
        "    ]  \n",
        ")  \n",
        "verification_questions = response_verification.choices[0].message.content  \n",
        "print(\"\\nVerification Questions:\\n\", verification_questions)  \n",
        "  \n",
        "# Step 3: Execute each verification question with context (simulate external search results)  \n",
        "search_context = \"\"\"  \n",
        "[1] 1846-1848  \n",
        "[2] 1845  \n",
        "[3] 1836  \n",
        "\"\"\"  \n",
        "  \n",
        "execute_verification_prompt = f\"\"\"Answer the following questions correctly based on the provided context. Think step by step and provide verified answers.  \n",
        "  \n",
        "Questions:  \n",
        "{verification_questions}  \n",
        "  \n",
        "Context: {search_context}  \n",
        "\"\"\"  \n",
        "  \n",
        "response_execute = client.chat.completions.create(  \n",
        "    model=\"gpt-4o\",  \n",
        "    messages=[  \n",
        "        {\"role\": \"system\", \"content\": system_message},  \n",
        "        {\"role\": \"user\", \"content\": execute_verification_prompt}  \n",
        "    ]  \n",
        ")  \n",
        "verified_answers = response_execute.choices[0].message.content  \n",
        "print(\"\\nVerified Answers:\\n\", verified_answers)  \n",
        "  \n",
        "# Step 4: Refine the baseline answer  \n",
        "final_prompt = f\"\"\"Based on the verified answers below, revise the original answer to include only facts verified by context. Remove any unsupported or incorrect details.  \n",
        "  \n",
        "Original Answer:  \n",
        "{baseline_answer}  \n",
        "  \n",
        "Verified Facts:  \n",
        "{verified_answers}  \n",
        "\"\"\"  \n",
        "  \n",
        "response_final = client.chat.completions.create(  \n",
        "    model=\"gpt-4o\",  \n",
        "    messages=[  \n",
        "        {\"role\": \"system\", \"content\": system_message},  \n",
        "        {\"role\": \"user\", \"content\": final_prompt}  \n",
        "    ]  \n",
        ")  \n",
        "final_answer = response_final.choices[0].message.content  \n",
        "print(\"\\nFinal, Corrected Answer:\\n\", final_answer)  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Chain of Density (CoD)  \n",
        "  \n",
        "Chain of Density (CoD) is an advanced summarization technique that generates increasingly concise, entity-dense summaries of an article in several iterations. At each step:  \n",
        "1. The model identifies 1–3 informative entities missing from the previous summary.  \n",
        "2. The model rewrites the summary to include all prior and new entities—resulting in a denser text of equal length.  \n",
        "  \n",
        "Guidelines:  \n",
        "- The first summary should be long (~80 words), using general language.  \n",
        "- Each subsequent summary rewrites the prior to improve brevity and information density.  \n",
        "- Each iteration adds the newly identified \"missing entities\" to the summary.  \n",
        "- Never remove entities from previous summaries; always include them in subsequent versions.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json  \n",
        "import re  \n",
        "  \n",
        "# System prompt for Chain of Density  \n",
        "cod_system_prompt = \"\"\"  \n",
        "You will generate increasingly concise, entity-dense summaries of an article given to you.  \n",
        "Repeat the following two steps 5 times:  \n",
        "  \n",
        "Step 1. Identify 1-3 informative entities (\", \" delimited) from the article which are missing from the previously generated summary.  \n",
        "Step 2. Write a new, denser summary of identical length which covers every entity and detail from the previous summary plus the missing entities.  \n",
        "  \n",
        "A missing entity is:  \n",
        "- relevant to the main story,  \n",
        "- specific yet concise (5 words or fewer),  \n",
        "- novel (not in the previous summary),  \n",
        "- faithful (present in the article),  \n",
        "- anywhere (can be located anywhere in the article).  \n",
        "  \n",
        "Guidelines:  \n",
        "- The first summary should be long (4-5 sentences, ~80 words) yet highly non-specific, containing little information beyond the entities marked as missing. Use overly generic language and fillers (e.g., \"this article discusses\") to reach ~80 words.  \n",
        "- Make every word count: rewrite the previous summary to improve flow and make space for additional entities.  \n",
        "- Make space with fusion, compression, and removal of uninformative phrases like \"the article discusses\".  \n",
        "- The summaries should become highly concise yet self-contained, i.e., easily understood without the article.  \n",
        "- Missing entities can appear anywhere in the summary.  \n",
        "- Never drop entities from the previous summary. If space cannot be made, add fewer new entities.  \n",
        "- Use the exact same number of words for each summary.  \n",
        "  \n",
        "Answer in JSON. The JSON should be a list (length 5) of dictionaries whose keys are 'Missing_Entities' and 'Denser_Summary'.  \n",
        "\"\"\"  \n",
        "  \n",
        "article = \"\"\"  \n",
        "This article discusses an incident that occurred during the Chinese Grand Prix involving two racing drivers, Jenson Button and Pastor Maldonado. The two were competing for the 13th place when Button collided with Maldonado's vehicle, causing damage to both cars. The incident resulted in a penalty for Button, who was demoted to 14th place. Maldonado, on the other hand, had to retire from the race due to the damage his car sustained. During the race, Jenson Button was driving for McLaren, and Pastor Maldonado for Lotus. Fernando Alonso witnessed the collision and managed to avoid it, advancing two places. Button received a five-second penalty and two penalty points on his super license. The incident caused front wing damage to Button's car and rear-end damage to Maldonado's. Nico Rosberg's Mercedes and Alonso's Ferrari were also involved; Sebastian Vettel and Kimi Raikkonen lapped Button during the aftermath. The crash happened on lap 49 of the incident-packed race.  \n",
        "\"\"\"  \n",
        "  \n",
        "# Prepare the user message for the model  \n",
        "initial_user_message = f\"\"\"Article:  \n",
        "{article}  \n",
        "  \n",
        "Please start the Chain of Density summarization process as described in the system prompt. Output as JSON.\"\"\"  \n",
        "  \n",
        "# Get the model's output  \n",
        "response_cod = client.chat.completions.create(  \n",
        "    model=\"gpt-4o\",  \n",
        "    messages=[  \n",
        "        {\"role\": \"system\", \"content\": cod_system_prompt},  \n",
        "        {\"role\": \"user\", \"content\": initial_user_message}  \n",
        "    ]  \n",
        ")  \n",
        "  \n",
        "# Extract the raw output and clean up from any extra markdown/text  \n",
        "raw_out = response_cod.choices[0].message.content.strip()  \n",
        "  \n",
        "def extract_json(raw_string):  \n",
        "    \"\"\"  \n",
        "    Extract a JSON array/dict from possibly noisy string using regex.  \n",
        "    \"\"\"  \n",
        "    # Look for the first '[' and the last ']', for a JSON array  \n",
        "    match = re.search(r'(\\[.*\\])', raw_string, re.DOTALL)  \n",
        "    if match:  \n",
        "        return match.group(1)  \n",
        "    # Look for curly braces for a dict  \n",
        "    match = re.search(r'(\\{.*\\})', raw_string, re.DOTALL)  \n",
        "    if match:  \n",
        "        return match.group(1)  \n",
        "    # Fallback: return raw string  \n",
        "    return raw_string  \n",
        "  \n",
        "# Try parsing the cleaned output as JSON  \n",
        "try:  \n",
        "    cod_json = json.loads(extract_json(raw_out))  \n",
        "    print(json.dumps(cod_json, indent=2))  \n",
        "except json.JSONDecodeError:  \n",
        "    print(\"❗️Model output could not be converted to JSON. Here is the raw text for debugging:\")  \n",
        "    print(raw_out)  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Self-Consistency (Majority Voting)  \n",
        "  \n",
        "**Enhancing Chain of Thought with Multiple Reasoning Paths**  \n",
        "  \n",
        "Self-Consistency (Majority Voting) improves reliability for complex reasoning by:  \n",
        "1. **Sampling the language model multiple times with chain-of-thought prompting** (not just using the first or \"greedy\" result).  \n",
        "2. **Aggregating the most consistent numeric answer among all outputs.**  \n",
        "  \n",
        "**Steps:**  \n",
        "1. Prompt the language model with a reasoning question using chain-of-thought.  \n",
        "2. Run the model several times (e.g., 5-10) to get diverse reasoning paths and answers.  \n",
        "3. Select the most frequent final answer (\"majority vote\") as the self-consistent solution.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import re  \n",
        "from collections import Counter  \n",
        "  \n",
        "# Chain-of-thought prompt  \n",
        "prompt = \"\"\"Q: Janet’s ducks lay 16 eggs per day. She eats three for breakfast every morning and bakes muffins for her friends every day with four. She sells the remainder for $2 per egg. How much does she make every day?  \n",
        "A:\"\"\"  \n",
        "  \n",
        "system_message = \"You are a helpful assistant. Answer showing step-by-step reasoning.\"  \n",
        "  \n",
        "def get_final_numeric_answer(text):  \n",
        "    \"\"\"  \n",
        "    Extract the final '$' answer from the model's output.  \n",
        "    \"\"\"  \n",
        "    match = re.search(r'The answer is \\$?(\\d+)\\$?', text)  \n",
        "    if match:  \n",
        "        return match.group(1)  \n",
        "    # Fallback: look for $X (e.g. $18)  \n",
        "    match = re.search(r'\\$(\\d+)', text)  \n",
        "    if match:  \n",
        "        return match.group(1)  \n",
        "    return None  \n",
        "  \n",
        "# Sample the model N times to get different reasoning paths  \n",
        "answers = []  \n",
        "n_samples = 8  # Try a few samples; you can increase for more robust voting  \n",
        "  \n",
        "for _ in range(n_samples):  \n",
        "    response = client.chat.completions.create(  \n",
        "        model=\"gpt-4o\",  \n",
        "        messages=[  \n",
        "            {\"role\": \"system\", \"content\": system_message},  \n",
        "            {\"role\": \"user\", \"content\": prompt}  \n",
        "        ],  \n",
        "        temperature=0.8  # Higher temp => more diverse paths  \n",
        "    )  \n",
        "    output = response.choices[0].message.content.strip()  \n",
        "    final = get_final_numeric_answer(output)  \n",
        "    answers.append(final)  \n",
        "    print(f\"Chain-of-thought Output:\\n{output}\\n\")  \n",
        "  \n",
        "# Majority voting (self-consistency)  \n",
        "filtered = [a for a in answers if a is not None]  \n",
        "freq = Counter(filtered)  \n",
        "most_common, freq_count = freq.most_common(1)[0]  \n",
        "  \n",
        "print(\"=\"*40)  \n",
        "print(f\"Answers: {answers}\")  \n",
        "print(f\"Self-Consistent (majority-vote) answer: ${most_common} ({freq_count}/{n_samples} times)\")  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# FLARE: Overview  \n",
        "  \n",
        "**FLARE (Forward-Looking Active Retrieval Augmented Generation)** is a process where a language model generates text step-by-step, and at each uncertain or knowledge-intensive point, it issues a search query, retrieves external evidence, and incorporates that information before continuing. This greatly reduces hallucinations and improves factuality.  \n",
        "  \n",
        "---  \n",
        "  \n",
        "## Example: Satya Nadella Biography Summary  \n",
        "  \n",
        "### FLARE Steps  \n",
        "  \n",
        "**1. Task Definition:**    \n",
        "Generate a fact-rich summary about Satya Nadella.  \n",
        "  \n",
        "**2. Initial Generation:**    \n",
        "Model starts:    \n",
        "_\"Satya Nadella is a prominent technology executive known for leading Microsoft.\"_  \n",
        "  \n",
        "**3. Forward-Looking Next Phrase Prediction:**    \n",
        "The model predicts:    \n",
        "_\"He completed his undergraduate studies in...\"_  \n",
        "  \n",
        "**4. Active Retrieval:**    \n",
        "Use \"**Satya Nadella education**\" as a search query.  \n",
        "  \n",
        "**5. Retrieval Results:**    \n",
        "- Bachelor's in Electrical Engineering from Manipal Institute of Technology  \n",
        "- Master's in Computer Science from University of Wisconsin–Milwaukee  \n",
        "- MBA from University of Chicago Booth School of Business  \n",
        "  \n",
        "**6. Augment Generation:**    \n",
        "_Incorporate facts:_    \n",
        "_\"He earned a bachelor's degree in electrical engineering from Manipal Institute of Technology, his master’s from the University of Wisconsin–Milwaukee, and an MBA from the University of Chicago Booth School of Business.\"_  \n",
        "  \n",
        "**7. Continue Iterating:**    \n",
        "Next phrase: \"_Before becoming CEO of Microsoft, Nadella..._\"    \n",
        "Query: \"**Satya Nadella Microsoft career**\", retrieve new facts, augment, and repeat until the summary is complete.  \n",
        "  \n",
        "---  \n",
        "  \n",
        "## Key FLARE Benefits  \n",
        "  \n",
        "- Reduces hallucinated or outdated facts  \n",
        "- Forces \"just-in-time\" knowledge integration at generation-time  \n",
        "- Can be applied to any LLM with a retrieval interface  \n",
        "  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import time  \n",
        "  \n",
        "# --- Simulated retrieval function ---  \n",
        "def search(query):  \n",
        "    \"\"\"Return simulated search results for given query.\"\"\"  \n",
        "    knowledge_base = {  \n",
        "        \"Satya Nadella education\": [  \n",
        "            \"Bachelor's in Electrical Engineering from Manipal Institute of Technology\",  \n",
        "            \"Master's in Computer Science from University of Wisconsin–Milwaukee\",  \n",
        "            \"MBA from University of Chicago Booth School of Business\"  \n",
        "        ],  \n",
        "        \"Satya Nadella Microsoft career\": [  \n",
        "            \"Started at Microsoft in 1992\",  \n",
        "            \"Led cloud and enterprise group\",  \n",
        "            \"Became CEO of Microsoft in 2014\"  \n",
        "        ]  \n",
        "    }  \n",
        "    print(f\"[Searching: '{query}'] ...\")  \n",
        "    time.sleep(1)  # Simulate search time  \n",
        "    return knowledge_base.get(query, [\"No results found\"])  \n",
        "  \n",
        "# --- FLARE-style generation steps ---  \n",
        "summary = \"Satya Nadella is a prominent technology executive known for leading Microsoft.\"  \n",
        "  \n",
        "# Step 1: Education  \n",
        "next_phrase = \"He completed his undergraduate studies in...\"  \n",
        "search_results = search(\"Satya Nadella education\")  \n",
        "education_facts = \" \".join(search_results)  \n",
        "summary += f\" {education_facts}\"  \n",
        "  \n",
        "# Step 2: Microsoft Career  \n",
        "next_phrase = \"Before becoming CEO of Microsoft, Nadella...\"  \n",
        "search_results = search(\"Satya Nadella Microsoft career\")  \n",
        "career_facts = \" \".join(search_results)  \n",
        "summary += f\" {career_facts}\"  \n",
        "  \n",
        "print(\"\\nFLARE-augmented Summary:\\n\")  \n",
        "print(summary)  "
      ]
    }
  ],
  "metadata": {
    "kernel_info": {
      "name": "python310-sdkv2"
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    },
    "microsoft": {
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      },
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      }
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
